#!/bin/sh
#############################################################################
#  Copyright (C) 2013-2015 Lawrence Livermore National Security, LLC.
#  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
#  Written by Albert Chu <chu11@llnl.gov>
#  LLNL-CODE-644248
#
#  This file is part of Magpie, scripts for running Hadoop on
#  traditional HPC systems.  For details, see https://github.com/llnl/magpie.
#
#  Magpie is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#
#  Magpie is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#
#  You should have received a copy of the GNU General Public License
#  along with Magpie.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

############################################################################
# LSF Customizations
############################################################################

# Node count.  Node count should include one node for the
# head/management/master node.  For example, if you want 8 compute
# nodes to process data, specify 9 nodes below.
#
# If including Zookeeper, include expected Zookeeper nodes.  For
# example, if you want 8 Hadoop compute nodes and 3 Zookeeper nodes,
# specify 12 nodes (1 master, 8 Hadoop, 3 Zookeeper)
#
# Also take into account additional nodes needed for other services,
# for example HDFS federation.

#BSUB -n <my node count>
#BSUB -o "lsf-%J.out"

# Note defaults of MAGPIE_STARTUP_TIME & MAGPIE_SHUTDOWN_TIME, this
# timelimit should be a fair amount larger than them combined.
#BSUB -W <my time in hours:minutes>

# Job name.  This will be used in naming directories for the job.
#BSUB -J <my job name>

# Queue to launch job in
#BSUB -q <my queue>

## LSF Values
# Generally speaking, don't touch these, just need to configure slurm

#BSUB -R "span[ptile=1]"
#BSUB -x

# Need to tell Magpie how you are submitting this job
export MAGPIE_SUBMISSION_TYPE="lsfmpirun"

############################################################################
# Magpie Configurations
############################################################################

# Directory your launching scripts/files are stored
#
# Normally an NFS mount, someplace magpie can be reached on all nodes.
export MAGPIE_SCRIPTS_HOME="${HOME}/magpie"

# Path to store data local to each cluster node, typically something
# in /tmp.  This will store local conf files and log files for your
# job.  If local scratch space is not available, consider using the
# MAGPIE_NO_LOCAL_DIR option.  See README for more details.
#
export MAGPIE_LOCAL_DIR="/tmp/${USER}/magpie"

# Magpie job type
#
# "hadoop" - Run a job according to the settings of HADOOP_MODE.
#
# "hbase" - Run a job according to the settings of HBASE_MODE.
#
# "phoenix" - Run a job according to the settings of PHOENIX_MODE
#
# "pig" - Run a job according to the settings of PIG_MODE.
#
# "mahout" - Run a job according to the settings of MAHOUT_MODE.
#
# "spark" - Run a job according to the settings of SPARK_MODE.
#
# "kafka" - Run a job according to the settings of KAFKA_MODE.
#
# "zeppelin" - Run a job according to the settings of ZEPPELIN_MODE.
#
# "storm" - Run a job according to the settings of STORM_MODE.
#
# "tachyon" - Run a job according ot the settings of TACHYON_MODE.
#
# "zookeeper" - Run a job according to the settings of ZOOKEEPER_MODE.
#
# "testall" - Run a job that runs all basic sanity tests for all
#             software that is configured to be setup.  This is a good
#             way to sanity check that everything has been setup
#             correctly and the way you like.
#
#             For Hadoop, testall will run terasort
#             For Hbase, testall will run performanceeval
#             For Phoenix, testall will run performanceeval
#             For Pig, testall will run testpig
#             For Mahout, testall will run clustersyntheticcontrol
#             For Spark, testall will run sparkpi
#             For Kafka, testall will run performance
#             For Storm, testall will run stormwordcount
#             For tachyon, testall will run testtachyon
#             For Zookeeper, testall will run zookeeperruok
#
# "script" - Run an arbitraty script, as specified by
#            MAGPIE_SCRIPT_PATH.  This functionally is very similar to
#            setting "script" in HADOOP_MODE or HBASE_MODE or
#            SPARK_MODE or STORM_MODE.
#
#            It is primarily used if you want to launch without
#            Hadoop/Hbase/Spark/Storm (such as Zookeeper only) and are
#            experimenting with things..
#
# "interactive" - manually interact with job run.  This functionally
#                 is very similar to setting "interactive" in
#                 HADOOP_MODE, HBASE_MODE, SPARK_MODE, STORM_MODE, or
#                 PIG_MODE.  It is primarily used if you want to
#                 launch without Hadoop/Hbase/Spark/Storm (such as
#                 Zookeeper only) and are experimenting with things..
#
export MAGPIE_JOB_TYPE="storm"

# Specify script to execute for "script" mode in MAGPIE_JOB_TYPE
#
# export MAGPIE_SCRIPT_PATH="${HOME}/my-job-script"

# Specify script startup / shutdown time window
#
# Specifies the amount of time to give startup / shutdown activities a
# chance to succeed before Magpie will give up (or in the case of
# shutdown, when the resource manager/scheduler may kill the running
# job).  Defaults to 30 minutes for startup, 30 minutes for shutdown.
#
# The startup time in particular may need to be increased if you have
# a large amount of data.  As an example, HDFS may need to spend a
# significant amount of time determine all of the blocks in HDFS
# before leaving safemode.
#
# The stop time in particular may need to be increased if you have a
# large amount of cleanup to be done.  HDFS will save its NameSpace
# before shutting down.  Hbase will do a compaction before shutting
# down.
#
# The startup & shutdown window must together be smaller than the
# SBATCH_TIMELIMIT specified above.
#
# MAGPIE_STARTUP_TIME and MAGPIE_SHUTDOWN_TIME at minimum must be 5
# minutes.  If MAGPIE_POST_JOB_RUN is specified below,
# MAGPIE_SHUTDOWN_TIME must be at minimum 10 minutes.
#
# export MAGPIE_STARTUP_TIME=30
# export MAGPIE_SHUTDOWN_TIME=30

# Convenience Scripts
#
# Specify script to be executed to before / after your job.  It is run
# on all nodes.
#
# Typically the pre-job script is used to set something up or get
# debugging info.  It can also be used to determine if system
# conditions meet the expectations of your job.  The primary job
# running script (magpie-run) will not be executed if the
# MAGPIE_PRE_JOB_RUN exits with a non-zero exit code.
#
# The post-job script is typically used for cleaning up something or
# gathering info (such as logs) for post-debugging/analysis.  If it is
# set, MAGPIE_SHUTDOWN_TIME above must be > 5.
#
# See example magpie-example-pre-job-script and
# magpie-example-post-job-script for ideas of what you can do w/ these
# scripts
#
# A number of convenient scripts are available in the scripts
# directory.
#
# export MAGPIE_PRE_JOB_RUN="${HOME}/magpie-my-pre-job-script"
# export MAGPIE_POST_JOB_RUN="${HOME}/magpie-my-post-job-script"

# Environment Variable Script
#
# When working with Magpie interactively by logging into the master
# node of your job allocation, many environment variables may need to
# be set.  For example, environment variables for config file
# directories (e.g. HADOOP_CONF_DIR, HBASE_CONF_DIR, etc.) and home
# directories (e.g. HADOOP_HOME, HBASE_HOME, etc.) and more general
# environment variables (e.g. JAVA_HOME) may need to be set before you
# begin interacting with your big data setup.
#
# The standard job output from Magpie provides instructions on all the
# environment variables typically needed to interact with your job.
# However, this can be tedious if done by hand.
#
# If the environment variable specified below is set, Magpie will
# create the file and put into it every environment variable that
# would be useful when running your job interactively.  That way, it
# can be sourced easily if you will be running your job interactively.
# It can also be loaded or used by other job scripts.
#
# export MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT="${HOME}/my-job-env"

# Environment Variable Shell Type
#
# Magpie outputs environment variables in help output and
# MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT based on your SHELL environment
# variable.
#
# If you would like to output in a different shell type (perhaps you
# have programmed scripts in a different shell), specify that shell
# here.
#
# export MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT_SHELL="/bin/bash"

# Remote Shell
#
# Magpie requires a passwordless remote shell command to launch
# necessary daemons across your job allocation.  Magpie defaults to
# ssh, but it may be an alternate command in some environments.  An
# alternate ssh-equivalent remote command can be specified by setting
# MAGPIE_REMOTE_CMD below.
#
# If using ssh, Magpie requires keys to be setup ahead of time so it
# can be executed without passwords.
#
# Specify options to the remote shell command if necessary.
#
# export MAGPIE_REMOTE_CMD="ssh"
# export MAGPIE_REMOTE_CMD_OPTS=""

############################################################################
# General Configuration
############################################################################

# Necessary for Hadoop, Hbase, Pig, and Zookeeper
export JAVA_HOME="/usr/lib/jvm/jre-1.7.0-oracle.x86_64/"

############################################################################
# Storm Core Configurations
############################################################################

# Should Storm be run
#
# Specify yes or no.  Defaults to no.
#
export STORM_SETUP=yes

# Version
#
export STORM_VERSION="0.9.6"

# Path to your Storm build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop/HDFS version this will run against.
#
export STORM_HOME="${HOME}/apache-storm-${STORM_VERSION}"

# Path to store data local to each cluster node, typically something
# in /tmp.  This will store local conf files and log files for your
# job.  If local scratch space is not available, consider using the
# MAGPIE_NO_LOCAL_DIR_DIR option.  See README for more details.
#
export STORM_LOCAL_DIR="/tmp/${USER}/storm"

# Directory where Storm configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export STORM_CONF_FILES="${HOME}/myconf"

# Storm Supervisor Slots
#
# Specify the number of slots/workers per supervisor/worker/slave shall run.
#
# If not specified, defaults to half the number of processors on the system.
#
# export STORM_SUPERVISOR_SLOTS="16"

# Daemon Heap Max
#
# Heap maximum for Storm daemons, specified in megs.
#
# If not specified, defaults to 1024
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export STORM_DAEMON_HEAP_MAX=1024

# Worker Heap Max
#
# Heap maximum for Storm workers, specified in megs.
#
# If not specified, defaults to 1024
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export STORM_WORKER_HEAP_MAX=1024

############################################################################
# Storm Job/Run Configurations
############################################################################

# Set how Storm should run
#
# "stormwordcount" - run Storm word count example.  Useful for making
#            sure things are setup the way you like.
#
#            Note that there is no "output" from stormwordcount, it is
#            stored in Storm log files.  Consider using the
#            magpie-gather-config-files-and-logs-script.sh to gather
#            those log files.
#
# "script" - execute a script that lists all of your Storm jobs.  Be
#            sure to set STORM_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse Storm, etc.
#                 In this mode you'll login to the cluster node that
#                 is your 'master' node and interact with Storm
#                 directly (e.g. bin/storm ...)
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup & teardown daemons themselves.
#
export STORM_MODE="stormwordcount"

############################################################################
# Storm Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See examples/storm-example-job-script for example of what to put in
# the script.
#
# export STORM_SCRIPT_PATH="${HOME}/my-job-script"

############################################################################
# Zookeeper Configurations
############################################################################

# Should Zookeeper be run
#
# Specify yes or no.  Defaults to no.
#
export ZOOKEEPER_SETUP=yes

# Zookeeper Replication Count
#
# Recommended to be odd.
#
export ZOOKEEPER_REPLICATION_COUNT=3

# Zookeeper Node Sharing
#
# By default, Zookeeper will not run on nodes that will run Hadoop/Hbase.
# They will have dedicated nodes for themselves.  If you do not wish
# for this to be the case, set the below to 'yes'.  Defaults to no.
#
# Keep in mind that adjustments to the number of nodes in your
# allocation may need to be adjusted given your setting of this
# parameter.  For example, if you want 8 nodes for Hadoop processing,
# you should increase your allocation by ZOOKEEPER_REPLICATION_COUNT
# if the below is 'no'.
#
# export ZOOKEEPER_SHARE_NODES=yes

# Set how Zookeeper should run
#
# "zookeeperruok" - Run a quick sanity test to see that zookeeper is
#             setup correctly.  zookeeperruok will do a simple 'ruok'
#             to all Zookeeper daemons.
#
# "launch" - Magpie will launch Zookeeper daemons
#
# "setuponly" - Like 'launch' but only setup conf files. useful
#               if user wants to setup & teardown daemons themselves.
#
export ZOOKEEPER_MODE="launch"

# Zookeeper Version
#
export ZOOKEEPER_VERSION="3.4.7"

# Path to your Zookeeper build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
export ZOOKEEPER_HOME="${HOME}/zookeeper-${ZOOKEEPER_VERSION}"

# Directory where Zookeeper configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export ZOOKEEPER_CONF_FILES="${HOME}/myconf"

# Path base for zookeeper data to be stored on each cluster node
#
# ZOOKEEPER_DATA_DIR can point to either a network file system path or
# a local drive path.
#
# If a local drive or SSD is available, a local path is preferable.
# If set to local, please see ZOOKEEPER_DATA_DIR_TYPE below for
# optimization possibilties.
#
export ZOOKEEPER_DATA_DIR="/lustre/${USER}/zookeeper"

# Zookeeper cleanup
#
# After your job has completed, if ZOOKEEPER_DATA_DIR_CLEAR is set to
# yes, Magpie will do a rm -rf on ZOOKEEPER_DATA_DIR.  This may be
# convenient for cleaning up your job after it has run.  This is
# particularly useful if ZOOKEEPER_DATA_DIR is on a local ssd /drive.
# B/c on your next job run, you may not be able to get the nodes you
# want on your next run, leading to problems.
#
# export ZOOKEEPER_DATA_DIR_CLEAR="yes"

# Zookeeper data dir type
#
# Inform Magpie what type of directory ZOOKEEPER_DATA_DIR points to.
#
# This configuration isn't entirely necessary to be set, but if set to
# networkfs, Magpie will increase a number of default timeouts in
# Zookeeper as well as other projects to adjust for the fact Zookeeper
# is running on a network file system.
#
# "networkfs" - ZOOKEEPER_DATA_DIR points to a network filesystem
#               (such as Lustre).
#
# "local" - ZOOKEEPER_DATA_DIR points to a local drive.
#
export ZOOKEEPER_DATA_DIR_TYPE="networkfs"

# Path to store data local to each cluster node, typically something
# in /tmp.  This will store local conf files and log files for your
# job.  If local scratch space is not available, consider using the
# MAGPIE_NO_LOCAL_DIR_DIR option.  See README for more details.
#
export ZOOKEEPER_LOCAL_DIR="/tmp/${USER}/zookeeper"

# Option to have per job data dir
#
# For each batch job, if this is set to yes, the location where
# zookeeper stores its data dir will be unique per job. The data
# dir will have the job id appended to the path to keep them organized.
# It will allows for multiple instances of the same script to be run
# without having collisions due to different nodes being used.
#
# export ZOOKEEPER_PER_JOB_DATA_DIR="yes"

# ZooKeeper ticktime, measured in milliseconds.  Used by all of
# Zookeeper for time measurement.
#
# Defaults to 2000.
#
# export ZOOKEEPER_TICKTIME=2000

# ZooKeeper initLimit, multiple of ticks to allow followers to connect
# and sync to a leader.  May need to increase this value if the data
# managed by ZooKeeper is large.
#
# Defaults to 10 if ZOOKEEPER_DATA_DIR_TYPE is local, 20 if networkfs
#
# export ZOOKEEPER_INITLIMIT=10

# ZooKeeper syncLimit, multiple of ticks to allow followers to sync
# with ZooKeeper.  If they fall too far behind a leader, they will be
# dropped.
#
# Defaults to 5 if ZOOKEEPER_DATA_DIR_TYPE is local, 10 if networkfs
#
# export ZOOKEEPER_SYNCLIMIT=5

############################################################################
# Run Job
############################################################################

# Set alternate mpirun options here
# MPIRUN_OPTIONS="-genvall -genv MV2_USE_APM 0"

mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-check-inputs
if [ $? -ne 0 ]
then
    exit 1
fi
mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-setup
mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-pre-run
if [ $? -ne 0 ]
then
    exit 1
fi
mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-run
mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-cleanup
mpirun $MPIRUN_OPTIONS $MAGPIE_SCRIPTS_HOME/magpie-post-run
