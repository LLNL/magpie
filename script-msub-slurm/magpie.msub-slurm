#!/bin/sh
#############################################################################
#  Copyright (C) 2013 Lawrence Livermore National Security, LLC.
#  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
#  Written by Albert Chu <chu11@llnl.gov>
#  LLNL-CODE-644248
#  
#  This file is part of Magpie, scripts for running Hadoop on
#  traditional HPC systems.  For details, see https://github.com/chu11/magpie.
#  
#  Magpie is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  Magpie is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with Magpie.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

############################################################################
# Moab Customizations
############################################################################

# Node count.  Node count should include one node for the
# head/management/master node.  For example, if you want 8 compute
# nodes to process data, specify 9 nodes below.
#
# If including Zookeeper, include expected Zookeeper nodes.  For
# example, if you want 8 Hadoop compute nodes and 3 Zookeeper nodes,
# specify 12 nodes (1 master, 8 Hadoop, 3 Zookeeper) 

#MSUB -l nodes=<my node count>

#MSUB -o ./moab-%j.out

#MSUB -l partition=<my partition> 

#MSUB -q <my batch queue>

# *** SEE MAGPIE_TIMELIMIT_MINUTES BELOW, IT MUST BE SET ***
#
# Note defaults of MAGPIE_STARTUP_TIME & MAGPIE_SHUTDOWN_TIME, the
# walltime should be a fair amount larger than them combined.

#MSUB -l walltime=<my time in seconds or HH:MM:SS>

#MSUB -l resfailpolicy=ignore

#MSUB -V

export MOAB_JOBNAME="<my job name>"

export SLURM_TASKS_PER_NODE=1
export SBATCH_EXCLUSIVE="yes"

# You MUST set the MAGPIE_TIMELIMIT_MINUTES value (in minutes) to
# match what is entered with walltime.  Moab does not export an
# environment variable with job time, so this must be done manually by
# the user.
# export MAGPIE_TIMELIMIT_MINUTES=<time in minutes>

# Need to tell Magpie how you are submitting this job
#
# IMPORTANT: This msub file assumes Slurm is the underlying resource
# manager.  If it is not, a new Magpie submission type should be added
# into Magpie.
export MAGPIE_SUBMISSION_TYPE="msubslurm"

############################################################################
# Magpie Configurations
############################################################################

# Directory your launching scripts/files are stored
#
# Normally an NFS mount, someplace magpie can be reached on all nodes.
export MAGPIE_SCRIPTS_HOME="/home/username/magpie"

# Path local to each cluster node, typically something in /tmp.
# This will store local scratch files for Magpie.
#
export MAGPIE_LOCAL_DIR="/tmp/username/magpie"

# Magpie job type
#
# "hadoop" - Run a job according to the settings of HADOOP_MODE.
#
# "hbase" - Run a job according to the settings of HBASE_MODE.
#
# "pig" - Run a job according to the settings of PIG_MODE.
#
# "spark" - Run a job according to the settings of SPARK_MODE.
#
# "storm" - Run a job according to the settings of STORM_MODE.
#
# "zookeeper" - Run a job according to the settings of ZOOKEEPER_MODE.
#
# "testall" - Run a job that runs all basic sanity tests for all
#             software that is configured to be setup.  This is a good
#             way to sanity check that everything has been setup
#             correctly and the way you like.
#
#             For Hadoop, testall will run terasort
#             For Hbase, testall will run performanceeval
#             For Pig, testall will run testpig
#             For Spark, testall will run sparkpi
#             For Storm, testall will run stormwordcount
#             For Zookeeper, testall will run zookeeperruok
#
# "script" - Run an arbitraty script, as specified by
#            MAGPIE_SCRIPT_PATH.  This functionally is very similar to
#            setting "script" in HADOOP_MODE or HBASE_MODE or
#            SPARK_MODE or STORM_MODE.
#
#            It is primarily used if you want to launch without
#            Hadoop/Hbase/Spark/Storm (such as Zookeeper only) and are
#            experimenting with things..
#
# "interactive" - manually interact with job run.  This functionally
#                 is very similar to setting "interactive" in
#                 HADOOP_MODE, HBASE_MODE, SPARK_MODE, STORM_MODE, or
#                 PIG_MODE.  It is primarily used if you want to
#                 launch without Hadoop/Hbase/Spark/Storm (such as
#                 Zookeeper only) and are experimenting with things..
#
export MAGPIE_JOB_TYPE="script"

# Specify script to execute for "script" mode in MAGPIE_JOB_TYPE
#
# export MAGPIE_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

# Specify script startup / shutdown time window
#
# Specifies the amount of time to give startup / shutdown activities a
# chance to succeed before Magpie will give up (or in the case of
# shutdown, when the resource manager/scheduler may kill the running
# job).  Defaults to 30 minutes for startup, 30 minutes for shutdown.
#
# The startup time in particular may need to be increased if you have
# a large amount of data.  As an example, HDFS may need to spend a
# significant amount of time determine all of the blocks in HDFS
# before leaving safemode.
#
# The stop time in particular may need to be increased if you have a
# large amount of cleanup to be done.  HDFS will save its NameSpace
# before shutting down.  Hbase will do a compaction before shutting
# down.
#
# The startup & shutdown window must together be smaller than the
# SBATCH_TIMELIMIT specified above.
#
# MAGPIE_STARTUP_TIME and MAGPIE_SHUTDOWN_TIME at minimum must be 5
# minutes.  If MAGPIE_POST_JOB_RUN is specified below,
# MAGPIE_SHUTDOWN_TIME must be at minimum 10 minutes.
#
# export MAGPIE_STARTUP_TIME=30
# export MAGPIE_SHUTDOWN_TIME=30

# Convenience Scripts
#
# Specify script to be executed to before / after your job.  It is run
# on all nodes.
#
# Typically the pre-job script is used to set something up or get
# debugging info.  It can also be used to determine if system
# conditions meet the expectations of your job.  The primary job
# running script (magpie-run) will not be executed if the
# MAGPIE_PRE_JOB_RUN exits with a non-zero exit code.
#
# The post-job script is typically used for cleaning up something or
# gathering info (such as logs) for post-debugging/analysis.  If it is
# set, MAGPIE_SHUTDOWN_TIME above must be > 5.
# 
# See example magpie-example-pre-job-script and
# magpie-example-post-job-script for ideas of what you can do w/ these
# scripts
#
# export MAGPIE_PRE_JOB_RUN="${MAGPIE_SCRIPTS_HOME}/magpie-my-pre-job-script"
# export MAGPIE_POST_JOB_RUN="${MAGPIE_SCRIPTS_HOME}/magpie-my-post-job-script"

# Environment Variable Script
#
# When working with Magpie interactively by logging into the master
# node of your job allocation, many environment variables may need to
# be set.  For example, environment variables for config file
# directories (e.g. HADOOP_CONF_DIR, HBASE_CONF_DIR, etc.) and log
# file directories (e.g. HADOOP_LOG_DIR, HBASE_LOG_DIR, etc.) and more
# general environment variables (e.g. JAVA_HOME) may need to be set
# before you begin interacting with your big data setup.
#
# The standard job output from Magpie provides instructions on all the
# environment variables typically needed to interact with your job.
# However, this can be tedious if done by hand.
#
# If the environment variable specified below is set, Magpie will
# create the file and put into it every environment variable that
# would be useful when running your job interactively.  That way, it
# can be sourced easily if you will be running your job interactively.
# It can also be loaded or used by other job scripts.
#
# export MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT="${MAGPIE_SCRIPTS_HOME}/my-job-env"

# Specify ssh-equivalent remote command and/or options if ssh is not
# available on your cluster
# export MAGPIE_REMOTE_CMD="mrsh"
# export MAGPIE_REMOTE_CMD_OPTS=""

############################################################################
# General Configuration
############################################################################

# Necessary for Hadoop, Hbase, Pig, and Zookeeper
export JAVA_HOME="/usr/lib/jvm/jre-1.6.0-sun.x86_64/"

############################################################################
# Hadoop Core Configurations
############################################################################

# Should Hadoop be run
#
# Specify yes or no.  Defaults to no.
# 
export HADOOP_SETUP=no

# Set Hadoop Setup Type
#
# Will inform scripts on how to setup config files and what daemons to
# launch/setup.  The hadoop build/binaries set by HADOOP_HOME
# needs to match up with what you set here.
#
# MR1 - MapReduce/Hadoop 1.0 w/ HDFS
# MR2 - MapReduce/Hadoop 2.0 w/ HDFS
# HDFS1 - HDFS only w/ Hadoop 1.0 
# HDFS2 - HDFS only w/ Hadoop 2.0
#
# The HDFS only options may be useful when you want to use HDFS with
# other big data software, such as Hbase, and do not care for using
# Hadoop MapReduce.  It only works with HDFS based
# HADOOP_FILESYSTEM_MODE, such as "hdfs", "hdfsoverlustre", or
# "hdfsovernetworkfs".
#
export HADOOP_SETUP_TYPE="MR2"

# Version
#
# Make sure the version for Mapreduce version 1 or 2 matches whatever
# you set in HADOOP_SETUP_TYPE
#
export HADOOP_VERSION="2.4.0"

# Path to your Hadoop build/binaries
#
# Make sure the build for MapReduce or HDFS version 1 or 2 matches
# whatever you set in HADOOP_SETUP_TYPE.
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
export HADOOP_HOME="/home/username/hadoop-${HADOOP_VERSION}"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
# This will not be used for storing intermediate files or
# distributed cache files.  See HADOOP_LOCALSTORE above for that.
#
export HADOOP_LOCAL_DIR="/tmp/username/hadoop"

# Directory where Hadoop configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export HADOOP_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Daemon Heap Max
#
# Heap maximum for Hadoop daemons (i.e. Resource Manger, NodeManager,
# DataNode, History Server, etc.), specified in megs.  Special case
# for Namenode, see below.
#
# If not specified, defaults to Hadoop default of 1000
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export HADOOP_DAEMON_HEAP_MAX=2000

# Daemon Namenode Heap Max
#
# Heap maximum for Hadoop Namenode daemons specified in megs.
#
# If not specified, defaults to HADOOP_DAEMON_HEAP_MAX above.
#
# Unlike most Hadoop daemons, namenode may need more memory if there
# are a very large number of files in your HDFS setup.  A general rule
# of thumb is a 1G heap for each 100T of data.
#
# export HADOOP_NAMENODE_DAEMON_HEAP_MAX=2000

# Environment Extra
#
# Specify extra environment information that should be passed into
# Hadoop.  This file will simply be appended into the hadoop-env.sh
# (and if appropriate) yarn-env.sh.
#
# By default, a reasonable estimate for max user processes and open
# file descriptors will be calculated and put into hadoop-env.sh (and
# if appropriate) yarn-env.sh.  However, it's always possible they may
# need to be set differently. Everyone's cluster/situation can be
# slightly different.
#
# See the example example-environment-extra extra for examples on
# what you can/should do with adding extra environment settings.
#
# export HADOOP_ENVIRONMENT_EXTRA_PATH="${MAGPIE_SCRIPTS_HOME}/hadoop-my-environment"

############################################################################
# Hadoop Job/Run Configurations
############################################################################

# Set how Hadoop should run
#
# "terasort" - run terasort.  Useful for making sure things are setup
#              the way you like.
#
#              There are additional configuration options for this
#              example listed below.
#
# "script" - execute a script that lists all of your Hadoop jobs.  Be
#            sure to set HADOOP_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse HDFS, etc.
#                 also useful for moving data in/out of HDFS.  In this
#                 mode you'll login to the cluster node that is your
#                 'master' node and interact with Hadoop directly
#                 (e.g. bin/hadoop ...)
#
# "upgradehdfs" - upgrade your version of HDFS.  Most notably this is
#                 used when you are switching to a newer Hadoop
#                 version and the HDFS version would be inconsistent
#                 without upgrading.  Only works with HDFS versions >=
#                 2.2.0.
#
#		  Beware, once you upgrade it'll be difficult to rollback.
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup daemons themselves, etc.
#
export HADOOP_MODE="terasort"

# Tasks per Node
#
# If not specified, a reasonable estimate will be calculated based on
# number of CPUs on the system.
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_MAX_TASKS_PER_NODE=8

# Default Map tasks for Job
#
# If not specified, defaults to HADOOP_MAX_TASKS_PER_NODE * compute
# nodes.
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_DEFAULT_MAP_TASKS=8

# Default Reduce tasks for Job
#
# If not specified, defaults to # compute nodes (i.e. 1 reducer per
# node)
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_DEFAULT_REDUCE_TASKS=8

# Max Map tasks for Task Tracker
#
# If not specified, defaults to HADOOP_MAX_TASKS_PER_NODE
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_MAX_MAP_TASKS=8

# Max Reduce tasks for Task Tracker
#
# If not specified, defaults to HADOOP_MAX_TASKS_PER_NODE
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_MAX_REDUCE_TASKS=8

# Heap size for JVM 
#
# Specified in M.  If not specified, a reasonable estimate will be
# calculated based on total memory available and number of CPUs on the
# system.
#
# HADOOP_CHILD_MAP_HEAPSIZE and HADOOP_CHILD_REDUCE_HEAPSIZE are for
# Yarn (i.e. HADOOP_SETUP_TYPE = MR2)
#
# If HADOOP_CHILD_MAP_HEAPSIZE is not specified, it is assumed to be
# HADOOP_CHILD_HEAPSIZE.
#
# If HADOOP_CHILD_REDUCE_HEAPSIZE is not specified, it is assumed to
# be 2X the HADOOP_CHILD_MAP_HEAPSIZE.
#
# If running Hbase (or other Big Data software) with Hadoop MapReduce,
# be aware of the number of tasks and the amount of memory that may be
# needed by other software.
#
# export HADOOP_CHILD_HEAPSIZE=2048
# export HADOOP_CHILD_MAP_HEAPSIZE=2048
# export HADOOP_CHILD_REDUCE_HEAPSIZE=4096

# Container Buffer
#
# Specify the amount of overhead each Yarn container will have over
# the heap size.  Specified in M.  If not specified, a reasonable
# estimate will be calculated based on total memory available.
#
# export HADOOP_CHILD_MAP_CONTAINER_BUFFER=256
# export HADOOP_CHILD_REDUCE_CONTAINER_BUFFER=512

# Mapreduce Slowstart, indicating percent of maps that should complete
# before reducers begin.
#
# If not specified, defaults to 0.05
#
# export HADOOP_MAPREDUCE_SLOWSTART=0.05

# Container Memory
#
# Memory on compute nodes for containers.  Typically "nice-chunk" less
# than actual memory on machine, b/c machine needs memory for its own
# needs (kernel, daemons, etc.).  Specified in megs.
#
# If not specified, a reasonable estimate will be calculated based on
# total memory on the system.
#
# export YARN_RESOURCE_MEMORY=32768

# Compression
#
# Should compression of outputs and intermediate data be enabled.
# Specify yes or no.  Defaults to no.
#
# Effectively, is time spend compressing data going to save you time
# on I/O.  Sometimes yes, sometimes no.
#
# export HADOOP_COMPRESSION=yes

# IO Sort Factors + MB
#
# The number of streams of files to sort while reducing and the memory
# amount to use while sorting.  This is a quite advanced mechanism
# taking into account many factors.  If not specified, some reasonable
# number will be calculated.
#
# export HADOOP_IO_SORT_FACTOR=10
# export HADOOP_IO_SORT_MB=100

# Parallel Copies
#
# The default number of parallel transfers run by reduce during the
# copy(shuffle) phase.  If not specified, some reasonable number will
# be calculated.
# export HADOOP_PARALLEL_COPIES=10

############################################################################
# Hadoop Filesystem Mode Configurations
############################################################################

# Set how the filesystem should be setup
#
# "hdfs" - Normal straight up HDFS if you have local disk in your
#          cluster.  This option is primarily for benchmarking and
#          probably shouldn't be used in the general case.
#
#          Be careful running this in a cluster environment.  The next
#          time you execute your job, if a different set of nodes are
#          allocated to you, the HDFS data you wrote from a previous
#          job may not be there.  Specifying specific nodes to use in
#          your job submission (e.g. --nodelist in sbatch) may be a
#          way to alleviate this.
#
#          User must set HADOOP_HDFSOVERLUSTRE_PATH below.
#
# "hdfsoverlustre" - HDFS over Lustre.  See README for description.
#
#                    User must set HADOOP_HDFSOVERLUSTRE_PATH below. 
#
# "hdfsovernetworkfs" - HDFS over Network FS.  Identical to HDFS over
#                       Lustre, but filesystem agnostic.
#
#                       User must set HADOOP_HDFSOVERNETWORKFS_PATH below.
#
# "rawnetworkfs" - Use Hadoop RawLocalFileSystem (i.e. file: scheme),
#           to use networked file system directly.  It could be a
#           Lustre mount or NFS mount.  Whatever you please.
#
#           User must set HADOOP_RAWNETWORKFS_PATH below.
#
export HADOOP_FILESYSTEM_MODE="hdfsoverlustre"

# Path for HDFS when using local disk
#
# If you want to specify multiple paths (such as multiple drives),
# make them comma separated (e.g. /dir1,/dir2,/dir3).  The multiple
# paths will be used for local intermediate data and HDFS.  The first
# path will also store daemon data, such as namenode or jobtracker
# data.
#
export HADOOP_HDFS_PATH="/ssd/username/"

# Lustre path to do Hadoop HDFS out of
#
# Note that different versions of Hadoop may not be compatible with
# your current HDFS data.  If you're going to switch around to
# different versions, perhaps set different paths for different data.
#
export HADOOP_HDFSOVERLUSTRE_PATH="/lustre/username/hdfsoverlustre/"

# HDFS over Lustre ignore lock
#
# Remove in_use.lock files before launching HDFS
#
# On traditional Hadoop clusters, the in_use.lock file protects
# against a second HDFS daemon running on the same node.  The lock
# file can similarly protect against a second HDFS daemon running on
# another node of your cluster.
#
# However, sometimes the lock file can be an annoyance.  If several
# nodes in your cluster die and you simply restart the job on
# separate nodes, the lock file may still be present and prohibit new
# HDFS daemons to run.
#
# By default, if this option is not set, the lock file will be left in
# place and may cause HDFS daemons to not start.  If set to yes, the
# lock files will be removed before starting HDFS.
#
# export HADOOP_HDFSOVERLUSTRE_REMOVE_LOCKS=yes

# Networkfs path to do Hadoop HDFS out of
#
# Note that different versions of Hadoop may not be compatible with
# your current HDFS data.  If you're going to switch around to
# different versions, perhaps set different paths for different data.
#
export HADOOP_HDFSOVERNETWORKFS_PATH="/networkfs/username/hdfsovernetworkfs/"

# HDFS over Networkfs ignore lock
#
# Remove in_use.lock files before launching HDFS
#
# On traditional Hadoop clusters, the in_use.lock file protects
# against a second HDFS daemon running on the same node.  The lock
# file can similarly protect against a second HDFS daemon running on
# another node of your cluster.
#
# However, sometimes the lock file can be an annoyance.  If several
# nodes in your cluster die and you simply restart the job on
# separate nodes, the lock file may still be present and prohibit new
# HDFS daemons to run.
#
# By default, if this option is not set, the lock file will be left in
# place and may cause HDFS daemons to not start.  If set to yes, the
# lock files will be removed before starting HDFS.
#
# export HADOOP_HDFSOVERNETWORKFS_REMOVE_LOCKS=yes

# Path for rawnetworkfs
#
# This path is used for creating local per-node paths.
#
export HADOOP_RAWNETWORKFS_PATH="/lustre/username/rawnetworkfs/"

# If you have a local SSD, performance may be better to store
# intermediate data on it rather than Lustre or some other networked
# filesystem.  If the below environment variable is specified, local
# intermediate data will be stored in the specified directory.
# Otherwise it will go to an appropriate directory in Lustre/networked
# FS.
#
# Be wary, local SSDs stores may have less space than HDDs or
# networked file systems.  It can be easy to run out of space.
#
# If you want to specify multiple paths (such as multiple drives),
# make them comma separated (e.g. /dir1,/dir2,/dir3).  The multiple
# paths will be used for local intermediate data.
#
# export HADOOP_LOCALSTORE="/ssd/username/localstore/"

# HDFS Block Size
#
# Commonly 134217728, 268435456, 536870912 (i.e. 128m, 256m, 512m)
#
# If not specified, defaults to 134217728
#
# export HADOOP_HDFS_BLOCKSIZE=134217728

# HDFS Replication
#
# HDFS commonly uses 3.  When doing HDFS over Lustre/NetworkFS, higher
# replication can also help with resilience if nodes fail.  You may
# wish to set this to < 3 to save space.
#
# If not specified, defaults to 3
#
# export HADOOP_HDFS_REPLICATION=3

# RawNetworkFS Block Size
#
# Commonly 33554432, 67108864, 134217728 (i.e. 32m, 64m, 128m)
#
# If not specified, defaults to 33554432
#
# export HADOOP_RAWNETWORKFS_BLOCKSIZE=33554432


############################################################################
# Hadoop Terasort Configurations
############################################################################

# Terasort size
#
# For "terasort" mode.
#
# Specify 10000000000 for terabyte, for actual benchmarking
#
# Specify something small, like 50000000 for basic sanity tests.
# Defaults to 50000000.
#
# export HADOOP_TERASORT_SIZE=50000000

# Terasort reducer count
#
# For "terasort" mode.
#
# If not specified, will be compute node count * 2.
#
# export HADOOP_TERASORT_REDUCER_COUNT=4

# Terasort cache
#
# For "real benchmarking" you should flush page cache between a
# teragen and a terasort.  You can disable this for sanity runs/tests
# to make things go faster.  Specify yes or no.  Defaults to yes.
#
# export HADOOP_TERASORT_CLEAR_CACHE=no

############################################################################
# Hadoop Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See hadoop-example-job-script for example of what to put in the script.
#
# export HADOOP_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

############################################################################
# Hadoop UDA Configurations
############################################################################

# Should Hadoop run with UDA
#
# Specify yes or no.  Defaults to no.
# 
export HADOOP_UDA_SETUP=no

# Specify UDA jar
#
export HADOOP_UDA_JAR="/home/username/uda/uda-hadoop-2.x.jar"

# Specify UDA shared object file
#
# Does not need to be specified, but if it is not installed in a
# common library path (e.g. /usr/lib), specify the path.
#
# export HADOOP_UDA_LIBPATH="/home/username/uda/lib/"

# Specify RDMA buffer size
#
# Specify in kilobytes, should generally be multiples of 1024.
# Defaults to 1024.
# 
# export HADOOP_UDA_RDMA_BUFFER_SIZE=1024

############################################################################
# Pig Configurations
############################################################################

# Should Pig be setup
#
# Specify yes or no.  Defaults to no.
# 
# Note that unlike Hadoop or Zookeeper, Pig does not need to be
# enabled/disabled to be run with Hadoop.  For example, no daemons are setup.  
#
# If PIG_SETUP is enabled, this will inform Magpie to setup a
# configuration files and environment variables that will hopefully
# make it easier to run Pig w/ Hadoop.  You could leave this disabled
# and setup/config Pig as you need.
#
export PIG_SETUP=no

# Pig Version
#
export PIG_VERSION="0.12.1"

# Path to your Pig build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop version this will run against.
#
export PIG_HOME="/home/username/pig-${PIG_VERSION}"

# Directory where Pig configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export PIG_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export PIG_LOCAL_DIR="/tmp/username/pig"

# Set how Pig should run
#
# "testpig" - Run a quick sanity test to see that pig is setup
#             correctly.  Testpig will do a simple ls of the root
#             directory of whatever filesytem you setup (e.g. hdfs,
#             lustre, etc.).
#
# "script" - execute a pig script indicated by PIG_SCRIPT_PATH.
#
# "interactive" - manually interact to run pig scripts. In this mode
#                 you'll login to the cluster node that is your
#                 'master' node and run pig scripts directly
#                 (e.g. bin/pig ...).
#
export PIG_MODE="testpig"

# Specify script to execute for "script" mode in PIG_MODE.  This pig
# script will be fed into pig via "pig ${PIG_SCRIPT_OPTS} ${PIG_SCRIPT_PATH}"
#
# export PIG_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-pig-script"

# Specify options to specify when calling pig.
#
# A particularly useful option is -l <logfile>, to indicate where you
# would like the pig log file to go.
#
# export PIG_SCRIPT_OPTS="-l /home/username/mypig.log"

# Pig Classpath
#
# Set the classpath you desire for pig.  This is the CLASSPATH you
# likely want to set to add additional jars for your run.
#
# export PIG_CLASSPATH="/home/username/myjarfiles.jar"

# Pig Opts
#
# Extra Java runtime options
#
# export PIG_OPTS=""

############################################################################
# Hbase Core Configurations
############################################################################

# Should Hbase be run
#
# Specify yes or no.  Defaults to no.
# 
export HBASE_SETUP=no

# Version
#
export HBASE_VERSION="0.98.3-hadoop2"

# Path to your Hbase build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop/HDFS version this will run against.
#
export HBASE_HOME="/home/username/hbase-${HBASE_VERSION}"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export HBASE_LOCAL_DIR="/tmp/username/hbase"

# Directory where Hbase configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export HBASE_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Master Daemon Heap Max
#
# Heap size for Hbase master daemons, specified in megs.
#
# If not specified, defaults to 1000
#
# export HBASE_MASTER_DAEMON_HEAP_MAX=1000

# Regionserver Daemon Heap Max
#
# Heap size for Hbase regionserver daemons, specified in megs.
#
# If not specified, defaults to 16000, or 50% of system memory,
# whichever is smaller.
#
# export HBASE_REGIONSERVER_DAEMON_HEAP_MAX=16000

# Environment Extra
#
# Specify extra environment information that should be passed into
# Hbase.  This file will simply be appended into the hbase-env.sh.
#
# By default, a reasonable estimate for max user processes and open
# file descriptors will be calculated and put into hbase-env.sh.
# However, it's always possible they may need to be set
# differently. Everyone's cluster/situation can be slightly different.
#
# See the example example-environment-extra for examples on
# what you can/should do with adding extra environment settings.
#
# export HBASE_ENVIRONMENT_EXTRA_PATH="${MAGPIE_SCRIPTS_HOME}/hbase-my-environment"

############################################################################
# Hbase Job/Run Configurations
############################################################################

# Set how Hbase should run
#
# "performanceeval" - run performance evaluation write and read.
#              Useful for making sure things are setup the way you
#              like.
#
#              There are additional configuration options for this
#              example listed below.
#
# "script" - execute a script that lists all of your Hbase jobs.  Be
#            sure to set HBASE_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse Hbase, etc.
#                 In this mode you'll login to the cluster node that
#                 is your 'master' node and interact with Hbase
#                 directly (e.g. bin/hbase ...)
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup daemons themselves, etc.
#
export HBASE_MODE="performanceeval"

# Should a major compaction be done when Hbase is being shut down.  In
# general, this is something that should be done to compact small
# files, remove flagged deletes, etc., but could take a long time.
# MAGPIE_SHUTDOWN_TIME should be large enough to handle this.
#
# This can also be run via the convenience script
# hbase-major-compaction.sh.
#
# Specify yes or no.  Defaults to yes.
# export HBASE_MAJOR_COMPACTION_ON_SHUTDOWN=yes

############################################################################
# Hbase Performance Evaluation Configurations
############################################################################

# Performance Evaluation Mode
#
# Specify 'sequential-thread' for threaded sequential write/read 
# Specify 'sequential-mr' for MapReduce sequential write/read
# Specify 'random-thread' for threaded random write/read 
# Specify 'random-mr' for MapReduce random write/read
#
# Defaults to 'sequential-thread' if not specified. 
# 
# export HBASE_PERFORMANCEEVAL_MODE="sequential-thread"

# Performance Evaluation Rows
#
# For "performanceeval" mode.  Rows each client will run.
#
# Defaults to 1048576 (1 million)
#
# export HBASE_PERFORMANCEEVAL_ROW_COUNT=1048576

# Performance Evaluation client count
#
# For "performanceeval" mode.
#
# If not specified, defaults to 1.
#
# export HBASE_PERFORMANCEEVAL_CLIENT_COUNT=1

############################################################################
# Hbase Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See hbase-example-job-script for example of what to put in the script.
#
# export HBASE_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

############################################################################
# Spark Core Configurations
############################################################################

# Should Spark be run
#
# Specify yes or no.  Defaults to no.
# 
export SPARK_SETUP=no

# Version
#
export SPARK_VERSION="1.0.0-bin-hadoop2"

# Path to your Spark build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop/HDFS version this will run against.
#
export SPARK_HOME="/home/username/spark-${SPARK_VERSION}"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export SPARK_LOCAL_DIR="/tmp/username/spark"

# Directory where Spark configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export SPARK_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Worker Cores per Node
#
# If not specified, a reasonable estimate will be calculated based on
# number of CPUs on the system.
#
# If also running Hbase or Hadoop MapReduce, be aware of the number of
# tasks and the amount of memory that may be needed by other software.
#
# export SPARK_WORKER_CORES_PER_NODE=8

# Worker Memory
#
# Specified in M.  If not specified, a reasonable estimate will be
# calculated based on total memory available and number of CPUs on the
# system.
#
# If also running Hbase or Hadoop MapReduce, be aware of the number of
# tasks and the amount of memory that may be needed by other software.
#
# export SPARK_WORKER_MEMORY_PER_NODE=16000

# Worker Directory
#
# Directory to run applications in, which will include both logs and
# scratch space for local jars.  If not specified, defaults to
# SPARK_LOCAL_DIR/work.
#
# Generally speaking, this is best if this is a tmp directory such as
# in /tmp
#
# export SPARK_WORKER_DIRECTORY=/tmp/username/spark/work

# SPARK_JOB_MEMORY
#
# Memory for spark jobs.  Defaults to being set equal to
# SPARK_WORKER_MEMORY_PER_NODE, but users may wish to lower it if
# multiple Spark jobs will be submitted at the same time.
#
# In Spark parlance, this will set both the executor and driver memory
# for Spark.
#
# export SPARK_JOB_MEMORY="2048"

# SPARK_DRIVER_MEMORY
#
# Beginning in Spark 1.0, driver memory could be configured separately
# from executor memory.  If SPARK_DRIVER_MEMORY is set below, driver
# memory will be configured differently than the executor memory
# indicated above with SPARK_JOB_MEMORY.
#
# If running Spark < 1.0, this option does nothing. 
#
# export SPARK_DRIVER_MEMORY="2048"

# Daemon Heap Max
#
# Heap maximum for Spark daemons, specified in megs.
#
# If not specified, defaults to 1000
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export SPARK_DAEMON_HEAP_MAX=2000

# Environment Extra
#
# Specify extra environment information that should be passed into
# Spark.  This file will simply be appended into the spark-env.sh.
#
# By default, a reasonable estimate for max user processes and open
# file descriptors will be calculated and put into spark-env.sh.
# However, it's always possible they may need to be set
# differently. Everyone's cluster/situation can be slightly different.
#
# See the example example-environment-extra for examples on
# what you can/should do with adding extra environment settings.
#
# export SPARK_ENVIRONMENT_EXTRA_PATH="${MAGPIE_SCRIPTS_HOME}/spark-my-environment"

############################################################################
# Spark Job/Run Configurations
############################################################################

# Set how Spark should run
#
# "sparkpi" - run sparkpi example. Useful for making sure things are
#             setup the way you like.
#
#             There are additional configuration options for this
#             example listed below.
#
# "sparkwordcount" - run wordcount example.  Useful for making sure
#                    things are setup the way you like.
#
#                    See below for additional required configuration
#                    for this example.
#
# "script" - execute a script that lists all of your Spark jobs.  Be
#            sure to set SPARK_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse Spark, etc.
#                 In this mode you'll login to the cluster node that
#                 is your 'master' node and interact with Spark
#                 directly (e.g. bin/spark-shell ...)
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup daemons themselves, etc.
#
export SPARK_MODE="sparkpi"

# SPARK_DEFAULT_PARALLELISM
#
# Default number of tasks to use across the cluster for distributed
# shuffle operations (groupByKey, reduceByKey, etc) when not set by
# user. If not specified, defaults to # compute nodes (i.e. 1 per node)
#
# export SPARK_DEFAULT_PARALLELISM=8

# SPARK_JOB_CLASSPATH
#
# May be necessary to set to get certain code/scripts to work.
#
# e.g. to run a Spark example, you may need to set
#
# export SPARK_JOB_CLASSPATH="examples/target/spark-examples-assembly-0.9.1.jar"
#
# export SPARK_JOB_CLASSPATH=""

# SPARK_JOB_LIBRARY_PATH
#
# May be necessary to set to get certain code/scripts to work.
#
# export SPARK_JOB_LIBRARY_PATH=""

# SPARK_JOB_JAVA_OPTS  
#
# May be necessary to set options to set Spark options
#
# e.g. -Dspark.default.parallelism=16
#
# Magpie will set several options on its own, however, these options
# will be appended last, ensuring they override anything that Magpie
# will set by default.
#
# Note that many of the options that were set in SPARK_JAVA_OPTS in
# Spark 0.9.1 and earlier have been moved to spark-defaults.conf.  It
# would be recommended to set alternate configs in there instead of
# here in Spark 1.0 and later.
#
# export SPARK_JOB_JAVA_OPTS=""

# SPARK_LOCAL_SCRATCH_DIR
#
# By default, if Hadoop is setup with a file system, the Spark local
# scratch directory, where scratch data is placed, will automatically
# be calculated and configured.  If Hadoop is not setup, the following
# must be specified.
#
# If you have local SSDs stored on the nodes of your system, it may be
# in your interest to set this to a local drive and adjust
# SPARK_LOCAL_SCRATCH_DIR_TYPE below to 'local'.
#
# export SPARK_LOCAL_SCRATCH_DIR="/myscratchlocation"

# Set how SPARK_LOCAL_SCRATCH_DIR should be setup.
#
# "networkfs" - SPARK_LOCAL_SCRATCH_DIR points to a network filesystem
#               (such as Lustre).  Local paths will be
#               setup.
#
# "local" - SPARK_LOCAL_SCRATCH_DIR points to a local drive.
#
# export SPARK_LOCAL_SCRATCH_DIR_TYPE="networkfs"

############################################################################
# Spark SparkPi Configuration
############################################################################

# SparkPi Slices
#
# Number of "slices" to parallelize in Pi estimation.  Generally
# speaking, more should lead to more accurate estimates.
#
# If not specified, equals number of nodes.
# 
# export SPARK_SPARKPI_SLICES=4

############################################################################
# Spark SparkWordCount Configuration
############################################################################

# SparkWordCount File
#
# Specify the file to do the word count on.  Specify the scheme, such
# as hdfs:// or file://, appropriately.
# 
# export SPARK_SPARKWORDCOUNT_FILE="/mywordcountfile"

# SparkWordCount Copy In File
#
# In some cases, a file must be copied in before it can be used.  Most
# notably, this can be the case if the file is not yet in HDFS.
#
# If specified below, the file will be copied to the location
# specified by SPARK_SPARKWORDCOUNT_FILE before the word count is
# executed.  
#
# Specify the scheme appropriately.  At this moment, the schemes of
# file:// and hdfs:// are recognized for this option.
#
# Note that this is not required.  The file could be copied in any
# number of other ways, such as through a previous job or through a
# script specified via MAGPIE_PRE_JOB_RUN.
#
# export SPARK_SPARKWORDCOUNT_COPY_IN_FILE="/mywordcountfile"

############################################################################
# Spark Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See spark-example-job-script for example of what to put in the script.
#
# export SPARK_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

############################################################################
# Storm Core Configurations
############################################################################

# Should Storm be run
#
# Specify yes or no.  Defaults to no.
# 
export STORM_SETUP=no

# Version
#
export STORM_VERSION="0.9.2-incubating"

# Path to your Storm build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
# Ensure the build matches the Hadoop/HDFS version this will run against.
#
export STORM_HOME="/home/username/apache-storm-${STORM_VERSION}"

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export STORM_LOCAL_DIR="/tmp/username/storm"

# Directory where Storm configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export STORM_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Storm Supervisor Slots
#
# Specify the number of slots/workers per supervisor/worker/slave shall run.
#
# If not specified, defaults to half the number of processors on the system.
#
# export STORM_SUPERVISOR_SLOTS="16"

# Daemon Heap Max
#
# Heap maximum for Storm daemons, specified in megs.
#
# If not specified, defaults to 1024
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export STORM_DAEMON_HEAP_MAX=1024

# Worker Heap Max
#
# Heap maximum for Storm workers, specified in megs.
#
# If not specified, defaults to 1024
#
# May need to be increased if you are scaling large, get OutofMemory
# errors, or perhaps have a lot of cores on a node.
#
# export STORM_WORKER_HEAP_MAX=1024

############################################################################
# Storm Job/Run Configurations
############################################################################

# Set how Storm should run
#
# "stormwordcount" - run Storm word count example.  Useful for making
#            sure things are setup the way you like.
#
# "script" - execute a script that lists all of your Storm jobs.  Be
#            sure to set STORM_SCRIPT_PATH to your script.
#
# "interactive" - manually interact to submit jobs, peruse Storm, etc.
#                 In this mode you'll login to the cluster node that
#                 is your 'master' node and interact with Storm
#                 directly (e.g. bin/storm ...)
#
# "setuponly" - Like 'interactive' but only setup conf files. useful
#               if user wants to setup daemons themselves, etc.
#
export STORM_MODE="stormwordcount"

############################################################################
# Storm Script Configurations
############################################################################

# Specify script to execute for "script" mode
#
# See storm-example-job-script for example of what to put in the script.
#
# export STORM_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

############################################################################
# Zookeeper Configurations
############################################################################

# Should Zookeeper be run
#
# Specify yes or no.  Defaults to no.
# 
export ZOOKEEPER_SETUP=no

# Zookeeper Replication Count
#
# Recommended to be odd.
#
export ZOOKEEPER_REPLICATION_COUNT=3

# Zookeeper Node Sharing
#
# By default, Zookeeper will not run on nodes that will run Hadoop/Hbase.
# They will have dedicated nodes for themselves.  If you do not wish
# for this to be the case, set the below to 'yes'.  Defaults to no.
#
# Keep in mind that adjustments to the number of nodes in your
# allocation may need to be adjusted given your setting of this
# parameter.  For example, if you want 8 nodes for Hadoop processing,
# you should increase your allocation by ZOOKEEPER_REPLICATION_COUNT
# if the below is 'no'.
#
# export ZOOKEEPER_SHARE_NODES=yes

# Set how Zookeeper should run
#
# "zookeeperruok" - Run a quick sanity test to see that zookeeper is
#             setup correctly.  zookeeperruok will do a simple 'ruok'
#             to all Zookeeper daemons.
#
# "launch" - Magpie will launch Zookeeper daemons
#
# "setuponly" - Only setup conf files. useful if user wants to setup
#               daemons themselves, etc.
#
export ZOOKEEPER_MODE="launch"

# Zookeeper Version
#
export ZOOKEEPER_VERSION="3.4.6"

# Path to your Zookeeper build/binaries
#
# This should be accessible on all nodes in your allocation. Typically
# this is in an NFS mount.
#
export ZOOKEEPER_HOME="/home/username/zookeeper-${ZOOKEEPER_VERSION}"

# Directory where Zookeeper configuration templates are stored
#
# If not specified, assumed to be $MAGPIE_SCRIPTS_HOME/conf
#
# export ZOOKEEPER_CONF_FILES="${MAGPIE_SCRIPTS_HOME}/conf"

# Set how Zookeeper data directory should be setup.
#
# "networkfs" - ZOOKEEPER_DATA_DIR points to a network filesystem
#               (such as Lustre).  Zookeeper local paths will be
#               setup.
#
# "local" - ZOOKEEPER_DATA_DIR points to a local drive.
#
export ZOOKEEPER_FILESYSTEM_MODE="networkfs"

# Path base for local data to each cluster node
#
# If ZOOKEEPER_FILESYSTEM_MODE is set to "networkfs", this is the base
# point for local data and per-node directories will be created
# (e.g. ${ZOOKEEPER_DATA_DIR}/node-1/)
#
# If you have local SSDs stored on the nodes of your system, it may be
# in your interest to set this to a local drive and adjust
# ZOOKEEPER_FILESYSTEM_MODE above to 'local'.
#
export ZOOKEEPER_DATA_DIR="/lustre/username/zookeeper"  

# Path local to each cluster node, typically something in /tmp.
# This will store local conf files and log files for your job. 
#
export ZOOKEEPER_LOCAL_DIR="/tmp/username/zookeeper"

# ZooKeeper ticktime, measured in milliseconds.  Used by all of
# Zookeeper for time measurement.
#
# Defaults to 2000.
#
# export ZOOKEEPER_TICKTIME=2000

# ZooKeeper initLimit, multiple of ticks to allow followers to connect
# and sync to a leader.  May need to increase this value if the data
# managed by ZooKeeper is large.
#
# Defaults to 10
#
# export ZOOKEEPER_INITLIMIT=10

# ZooKeeper syncLimit, multiple of ticks to allow followers to sync
# with ZooKeeper.  If they fall too far behind a leader, they will be
# dropped.
#
# Defaults to 5
#
# export ZOOKEEPER_SYNCLIMIT=5

############################################################################
# Run Job
############################################################################

srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-check-inputs
if [ $? -ne 0 ]
then
    exit 1
fi
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-core
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-hadoop
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-pig
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-zookeeper
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-hbase
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-spark
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-setup-storm
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-pre-run
if [ $? -ne 0 ]
then
    exit 1
fi
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-run
srun --no-kill -W 0 $MAGPIE_SCRIPTS_HOME/magpie-post-run
