<?xml version="1.0"?>
<configuration>

<!-- Site specific YARN configuration properties -->

<property>
  <description>The address of the applications manager interface in the RM.</description>
  <name>yarn.resourcemanager.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERADDRESS</value>
</property>

<property>
  <description>The address of the scheduler interface.</description>
  <name>yarn.resourcemanager.scheduler.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERSCHEDULERADDRESS</value>
</property>

<property>
  <description>The address of the RM web application.</description>
  <name>yarn.resourcemanager.webapp.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERWEBAPPADDRESS</value>
</property>

<property>
  <description>The address of the RM web application.</description>
  <name>yarn.resourcemanager.webapp.https.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERWEBAPPHTTPSADDRESS</value>
</property>

<property>
  <name>yarn.resourcemanager.resource-tracker.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERRESOURCETRACKERADDRESS</value>
</property>

<property>
  <description>The address of the RM admin interface.</description>
  <name>yarn.resourcemanager.admin.address</name>
  <value>HADOOP_MASTER_HOST:YARNRESOURCEMANAGERADMINADDRESS</value>
</property>

<property>
  <description>The minimum allocation for every container request at the RM,
    in MBs. Memory requests lower than this won't take effect,
    and the specified value will get allocated at minimum.</description>
  <name>yarn.scheduler.minimum-allocation-mb</name>
  <value>YARNMINCONTAINER</value>
</property>

<property>
  <description>The maximum allocation for every container request at the RM,
    in MBs. Memory requests higher than this won't take effect,
    and will get capped to this value.</description>
  <name>yarn.scheduler.maximum-allocation-mb</name>
  <value>YARNMAXCONTAINER</value>
</property>

<property>
  <description>Amount of physical memory, in MB, that can be allocated 
  for containers.</description>
  <name>yarn.nodemanager.resource.memory-mb</name>
  <value>YARNRESOURCEMEMORY</value>
</property>

<property>
  <description>Whether virtual memory limits will be enforced for
  containers.</description>
  <name>yarn.nodemanager.vmem-check-enabled</name>
  <value>YARNVMEMCHECK</value>
</property>

<property>
  <description>Whether physical memory limits will be enforced for
  containers.</description>
  <name>yarn.nodemanager.pmem-check-enabled</name>
  <value>YARNPMEMCHECK</value>
</property>

<property>
  <description>The minimum allocation for every container request at the RM,
  in terms of virtual CPU cores. Requests lower than this won't take effect,
  and the specified value will get allocated the minimum.</description>
  <name>yarn.scheduler.minimum-allocation-vcores</name>
  <value>1</value>
</property>

<property>
  <description>The maximum allocation for every container request at the RM,
  in terms of virtual CPU cores. Requests higher than this won't take effect,
  and will get capped to this value.</description>
  <name>yarn.scheduler.maximum-allocation-vcores</name>
  <value>YARNRESROUCECPUVCORES</value>
</property>

<property>
  <description>Number of vcores that can be allocated
  for containers. This is used by the RM scheduler when allocating
  resources for containers. This is not used to limit the number of
  physical cores used by YARN containers.</description>
  <name>yarn.nodemanager.resource.cpu-vcores</name>
  <value>YARNRESROUCECPUVCORES</value>
</property>

<property>
  <name>yarn.nodemanager.aux-services</name>
  <value>YARNAUXSERVICES</value>
</property>

<property>
  <name>YARNAUXMAPREDUCESHUFFLE</name>
  <value>org.apache.hadoop.mapred.ShuffleHandler</value>
</property>

<property>
  <description>Address where the localizer IPC is.</description>
  <name>yarn.nodemanager.localizer.address</name>
  <value>${yarn.nodemanager.hostname}:YARNLOCALIZERADDRESS</value>
</property>

<property>
  <description>NM Webapp address.</description>
  <name>yarn.nodemanager.webapp.address</name>
  <value>${yarn.nodemanager.hostname}:YARNNODEMANAGERWEBAPPADDRESS</value>
</property>

<property>
  <description>List of directories to store localized files in. An
    application's localized file directory will be found in:
    ${yarn.nodemanager.local-dirs}/usercache/${user}/appcache/application_${appid}.
    Individual containers' work directories, called container_${contid}, will
    be subdirectories of this.
  </description>
  <name>yarn.nodemanager.local-dirs</name>
  <value>LOCALSTOREDIR</value>
</property>

<property>
  <name>yarn.acl.enable</name>
  <value>true</value>
  <description>Are acls enabled.</description>
</property>

<property>
  <name>yarn.admin.acl</name>
  <value>YARNDEFAULTUSER</value>
  <description>ACL of who can be admin of the YARN cluster.</description>
</property>

<property>
  <name>yarn.scheduler.capacity.root.queues</name>
  <value>default</value>
  <description>The queues at the this level (root is the root queue).
  </description>
</property>

<property>
  <name>yarn.scheduler.capacity.root.acl_submit_applications</name>
  <value>YARNDEFAULTUSER</value>
</property>

<property>
  <name>yarn.scheduler.capacity.root.acl_administer_queue</name>
  <value>YARNDEFAULTUSER</value>
</property>

<property>
  <name>yarn.scheduler.capacity.root.default.capacity</name>
  <value>100</value>
</property>

<property>
  <name>yarn.scheduler.capacity.root.default.state</name>
  <value>RUNNING</value>
</property>

<property>
  <name>yarn.scheduler.capacity.root.default.acl_submit_applications</name>
  <value>YARNDEFAULTUSER</value>
</property>

<property>
  <name>yarn.scheduler.capacity.root.default.acl_administer_queue</name>
  <value>YARNDEFAULTUSER</value>
</property>

<property>
  <description>The maximum percentage of disk space utilization allowed after 
  which a disk is marked as bad. Values can range from 0.0 to 100.0. 
  If the value is greater than or equal to 100, the nodemanager will check 
  for full disk. This applies to yarn.nodemanager.local-dirs and
  yarn.nodemanager.log-dirs.</description>
  <name>yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage</name>
  <value>98.0</value>
</property>

</configuration>
