############################################################################
# Magpie Configurations
############################################################################

# Directory your launching scripts/files are stored
#
# Normally an NFS mount, someplace magpie can be reached on all nodes.
export MAGPIE_SCRIPTS_HOME="/home/username/magpie"

# Path local to each cluster node, typically something in /tmp.
# This will store local scratch files for Magpie.
#
export MAGPIE_LOCAL_DIR="/tmp/username/magpie"

# Magpie job type
#
# "hadoop" - Run a job according to the settings of HADOOP_MODE.
#
# "hbase" - Run a job according to the settings of HBASE_MODE.
#
# "pig" - Run a job according to the settings of PIG_MODE.
#
# "spark" - Run a job according to the settings of SPARK_MODE.
#
# "storm" - Run a job according to the settings of STORM_MODE.
#
# "zookeeper" - Run a job according to the settings of ZOOKEEPER_MODE.
#
# "testall" - Run a job that runs all basic sanity tests for all
#             software that is configured to be setup.  This is a good
#             way to sanity check that everything has been setup
#             correctly and the way you like.
#
#             For Hadoop, testall will run terasort
#             For Hbase, testall will run performanceeval
#             For Pig, testall will run testpig
#             For Spark, testall will run sparkpi
#             For Storm, testall will run stormwordcount
#             For Zookeeper, testall will run zookeeperruok
#
# "script" - Run an arbitraty script, as specified by
#            MAGPIE_SCRIPT_PATH.  This functionally is very similar to
#            setting "script" in HADOOP_MODE or HBASE_MODE or
#            SPARK_MODE or STORM_MODE.
#
#            It is primarily used if you want to launch without
#            Hadoop/Hbase/Spark/Storm (such as Zookeeper only) and are
#            experimenting with things..
#
# "interactive" - manually interact with job run.  This functionally
#                 is very similar to setting "interactive" in
#                 HADOOP_MODE, HBASE_MODE, SPARK_MODE, STORM_MODE, or
#                 PIG_MODE.  It is primarily used if you want to
#                 launch without Hadoop/Hbase/Spark/Storm (such as
#                 Zookeeper only) and are experimenting with things..
#
export MAGPIE_JOB_TYPE="script"

# Specify whether or not to run slave daemons on master node
#
# By default, the master node will only run master daemons, such as
# the Hadoop NameNode, Hbase Master, Spark Master, etc.
#
# On certain workloads the amount of memory and processing used on
# the master may be low enough it would be worthwhile to also run
# slave node daemons on the master (e.g. Hadoop DataNode, Hbase
# RegionServer, Spark Worker, etc.). By setting the below to yes,
# you will inform Magpie to also run slave node daemons on the
# master. If enabled, you are advised to adjust heap and memory
# usage of all necessary daemons to ensure that resources are
# allocated appropriately to ensure decent performance.
#
# By default, this is turned off.
#
# export MAGPIE_RUN_SLAVE_ON_MASTER="yes"

# Specify script to execute for "script" mode in MAGPIE_JOB_TYPE
#
# export MAGPIE_SCRIPT_PATH="${MAGPIE_SCRIPTS_HOME}/my-job-script"

# Specify script startup / shutdown time window
#
# Specifies the amount of time to give startup / shutdown activities a
# chance to succeed before Magpie will give up (or in the case of
# shutdown, when the resource manager/scheduler may kill the running
# job).  Defaults to 30 minutes for startup, 30 minutes for shutdown.
#
# The startup time in particular may need to be increased if you have
# a large amount of data.  As an example, HDFS may need to spend a
# significant amount of time determine all of the blocks in HDFS
# before leaving safemode.
#
# The stop time in particular may need to be increased if you have a
# large amount of cleanup to be done.  HDFS will save its NameSpace
# before shutting down.  Hbase will do a compaction before shutting
# down.
#
# The startup & shutdown window must together be smaller than the
# SBATCH_TIMELIMIT specified above.
#
# MAGPIE_STARTUP_TIME and MAGPIE_SHUTDOWN_TIME at minimum must be 5
# minutes.  If MAGPIE_POST_JOB_RUN is specified below,
# MAGPIE_SHUTDOWN_TIME must be at minimum 10 minutes.
#
# export MAGPIE_STARTUP_TIME=30
# export MAGPIE_SHUTDOWN_TIME=30

# Convenience Scripts
#
# Specify script to be executed to before / after your job.  It is run
# on all nodes.
#
# Typically the pre-job script is used to set something up or get
# debugging info.  It can also be used to determine if system
# conditions meet the expectations of your job.  The primary job
# running script (magpie-run) will not be executed if the
# MAGPIE_PRE_JOB_RUN exits with a non-zero exit code.
#
# The post-job script is typically used for cleaning up something or
# gathering info (such as logs) for post-debugging/analysis.  If it is
# set, MAGPIE_SHUTDOWN_TIME above must be > 5.
# 
# See example magpie-example-pre-job-script and
# magpie-example-post-job-script for ideas of what you can do w/ these
# scripts
#
# export MAGPIE_PRE_JOB_RUN="${MAGPIE_SCRIPTS_HOME}/magpie-my-pre-job-script"
# export MAGPIE_POST_JOB_RUN="${MAGPIE_SCRIPTS_HOME}/magpie-my-post-job-script"

# Environment Variable Script
#
# When working with Magpie interactively by logging into the master
# node of your job allocation, many environment variables may need to
# be set.  For example, environment variables for config file
# directories (e.g. HADOOP_CONF_DIR, HBASE_CONF_DIR, etc.) and home
# directories (e.g. HADOOP_HOME, HBASE_HOME, etc.) and more general
# environment variables (e.g. JAVA_HOME) may need to be set before you
# begin interacting with your big data setup.
#
# The standard job output from Magpie provides instructions on all the
# environment variables typically needed to interact with your job.
# However, this can be tedious if done by hand.
#
# If the environment variable specified below is set, Magpie will
# create the file and put into it every environment variable that
# would be useful when running your job interactively.  That way, it
# can be sourced easily if you will be running your job interactively.
# It can also be loaded or used by other job scripts.
#
# export MAGPIE_ENVIRONMENT_VARIABLE_SCRIPT="${MAGPIE_SCRIPTS_HOME}/my-job-env"

# Specify ssh-equivalent remote command and/or options if ssh is not
# available on your cluster
# export MAGPIE_REMOTE_CMD="mrsh"
# export MAGPIE_REMOTE_CMD_OPTS=""

