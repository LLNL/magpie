#!/bin/bash
#############################################################################
#  Copyright (C) 2013-2015 Lawrence Livermore National Security, LLC.
#  Produced at Lawrence Livermore National Laboratory (cf, DISCLAIMER).
#  Written by Albert Chu <chu11@llnl.gov>
#  LLNL-CODE-644248
#  
#  This file is part of Magpie, scripts for running Hadoop on
#  traditional HPC systems.  For details, see <URL>.
#  
#  Magpie is free software; you can redistribute it and/or modify it
#  under the terms of the GNU General Public License as published by
#  the Free Software Foundation; either version 2 of the License, or
#  (at your option) any later version.
#  
#  Magpie is distributed in the hope that it will be useful, but
#  WITHOUT ANY WARRANTY; without even the implied warranty of
#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
#  General Public License for more details.
#  
#  You should have received a copy of the GNU General Public License
#  along with Magpie.  If not, see <http://www.gnu.org/licenses/>.
#############################################################################

# This script sets up configuration files for jobs.  For the most
# part, it shouldn't be editted.  See job submission files for
# configuration details.

if [ "${HADOOP_SETUP}" != "yes" ]
then
    exit 0
fi

source ${MAGPIE_SCRIPTS_HOME}/magpie-submission-convert
source ${MAGPIE_SCRIPTS_HOME}/magpie-common-exports
source ${MAGPIE_SCRIPTS_HOME}/magpie-common-functions
source ${MAGPIE_SCRIPTS_HOME}/magpie-variable-conversion

# hadoopnoderank set if succeed
if ! Magpie_am_I_a_hadoop_node
then
    exit 0
fi

# For rest of setup, we will use cluster specific paths
Magpie_make_all_local_dirs_node_specific

extrahadoopclasses=""
extrahadoopopts=""
extrahadooptaskopts=""

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
then
    # For some reason Hadoop 1.0 needs FQDN hosts in hosts-include
    cp ${HADOOP_CONF_DIR}/slaves_fqdn ${HADOOP_CONF_DIR}/hosts-include
else
    cp ${HADOOP_CONF_DIR}/slaves ${HADOOP_CONF_DIR}/hosts-include
fi
hostsincludefile="${HADOOP_CONF_DIR}/hosts-include"
hostsincludefilesubst=`echo "${hostsincludefile}" | sed "s/\\//\\\\\\\\\//g"`

# By default leave exclude file empty
touch ${HADOOP_CONF_DIR}/hosts-exclude
hostsexcludefile="${HADOOP_CONF_DIR}/hosts-exclude"
hostsexcludefilesubst=`echo "${hostsexcludefile}" | sed "s/\\//\\\\\\\\\//g"`

#
# Calculate values for various config file variables, based on
# recommendtions, rules of thumb, or based on what user input.
#

# Recommendation from Cloudera, parallel copies sqrt(number of nodes), floor of ten
if [ "${HADOOP_PARALLEL_COPIES}X" != "X" ]
then
    parallelcopies=${HADOOP_PARALLEL_COPIES}
else 
    parallelcopies=$(echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f")
    if [ "${parallelcopies}" -lt "10" ]
    then
	parallelcopies=10
    fi
fi

# Recommendation from Cloudera, 10% of nodes w/ floor of ten, ceiling 200
# My experience this is low b/c of high core counts, so bump higher to 50% 
namenodehandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .5" | bc -l | xargs printf "%1.0f")
if [ "${namenodehandlercount}" -lt "10" ]
then
    namenodehandlercount=10
fi

if [ "${namenodehandlercount}" -gt "200" ]
then
    namenodehandlercount=200
fi

# General rule of thumb is half namenode handler count, so * .25 instead of * .5
datanodehandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .25" | bc -l | xargs printf "%1.0f")
if [ "${datanodehandlercount}" -lt "10" ]
then
    datanodehandlercount=10
fi

if [ "${datanodehandlercount}" -gt "200" ]
then
    datanodehandlercount=200
fi

# Per description, about 4% of nodes but w/ floor of 10
jobtrackerhandlercount=$(echo "${HADOOP_SLAVE_COUNT} * .04" | bc -l | xargs printf "%1.0f")
if [ "${jobtrackerhandlercount}" -lt "10" ]
then
    jobtrackerhandlercount=10
fi

# Per descrption, about square root number of nodes
submitfilereplication=$(echo "sqrt ( ${HADOOP_SLAVE_COUNT} )" | bc -l | xargs printf "%1.0f")

# Optimal depends on file system
if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    iobuffersize=65536
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    # Default block size is 1M in Lustre
    # XXX: If not default, can get from lctl or similar?
    # If other networkFS, just assume like Lustre
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    # Assuming Lustre, so copy above 1M
    iobuffersize=1048576
fi 

javahomesubst=`echo "${JAVA_HOME}" | sed "s/\\//\\\\\\\\\//g"`

# Sets threadstouse
Magpie_calculate_threadstouse

# Sets memorytouse
Magpie_calculate_memorytouse

if [ "${HADOOP_MAX_TASKS_PER_NODE}X" != "X" ]
then
    maxtaskspernode=${HADOOP_MAX_TASKS_PER_NODE}
else
    maxtaskspernode=${threadstouse}
fi

if [ "${YARN_RESOURCE_MEMORY}X" != "X" ]
then
    yarnresourcememory=${YARN_RESOURCE_MEMORY}
else
    yarnresourcememory=${memorytouse}
fi

if [ "${HADOOP_CHILD_HEAPSIZE}X" != "X" ]
then
    allchildheapsize=${HADOOP_CHILD_HEAPSIZE}
else
# achu: We round down to nearest 512M
    tmp1=$(echo "${yarnresourcememory} / ${maxtaskspernode}" | bc -l | xargs printf "%1.2f")
    tmp2=$(echo "${tmp1} / 512" | bc -l | xargs printf "%1.0f")
    allchildheapsize=$(echo "${tmp2} * 512" | bc -l | xargs printf "%1.0f")
    if [ "${allchildheapsize}" -lt "512" ]
    then
	allchildheapsize=512
    fi
fi

if [ "${HADOOP_CHILD_MAP_HEAPSIZE}X" != "X" ]
then
    mapchildheapsize=${HADOOP_CHILD_MAP_HEAPSIZE}
else
    mapchildheapsize=${allchildheapsize}
fi

if [ "${HADOOP_CHILD_REDUCE_HEAPSIZE}X" != "X" ]
then
    reducechildheapsize=${HADOOP_CHILD_REDUCE_HEAPSIZE}
else
    if [ "${HADOOP_SETUP_TYPE}" == "MR2" ]
    then
	reducechildheapsize=`expr ${mapchildheapsize} \* 2`
    else
	reducechildheapsize=${allchildheapsize}
    fi
fi

if [ "${HADOOP_CHILD_MAP_CONTAINER_BUFFER}X" != "X" ]
then
    mapcontainerbuffer=${HADOOP_CHILD_MAP_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    mapcontainerbuffer=`expr ${numgig} \* 256`
fi

if [ "${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}X" != "X" ]
then
    reducecontainerbuffer=${HADOOP_CHILD_REDUCE_CONTAINER_BUFFER}
else
    # Estimate 256M per G
    numgig=`expr ${reducechildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    reducecontainerbuffer=`expr ${numgig} \* 256`
fi

# Cloudera recommends 256 for io.sort.mb.  Cloudera blog suggests
# io.sort.factor * 10 ~= io.sort.mb.

if [ "${HADOOP_IO_SORT_MB}X" != "X" ]
then
    iosortmb=${HADOOP_IO_SORT_MB}
else
    # 128M per gig
    numgig=`expr ${mapchildheapsize} / 1024`
    if [ "${numgig}" == "0" ]
    then
	numgig=1
    fi
    iosortmb=`expr ${numgig} \* 128`
fi

if [ "${HADOOP_IO_SORT_FACTOR}X" != "X" ]
then
    iosortfactor=${HADOOP_IO_SORT_FACTOR}
else
    iosortfactor=`expr ${iosortmb} \/ 10`
fi

mapcontainermb=`expr ${mapchildheapsize} + ${mapcontainerbuffer}`
reducecontainermb=`expr ${reducechildheapsize} + ${reducecontainerbuffer}`

yarnmincontainer=1024
if [ ${mapcontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${mapcontainermb}
fi

if [ ${reducecontainermb} -lt ${yarnmincontainer} ]
then
    yarnmincontainer=${reducecontainermb}
fi

yarnmaxcontainer=8192
if [ ${mapcontainermb} -gt ${yarnmaxcontainer} ]
then
    yarnmaxcontainer=${mapcontainermb}
fi

if [ ${reducecontainermb} -gt ${yarnmaxcontainer} ]
then
    yarnmaxcontainer=${reducecontainermb}
fi

if [ "${HADOOP_MAPREDUCE_SLOWSTART}X" != "X" ]
then
    mapredslowstart=${HADOOP_MAPREDUCE_SLOWSTART}
else
    mapredslowstart="0.05"
fi

if [ "${HADOOP_DEFAULT_MAP_TASKS}X" != "X" ]
then
    defaultmaptasks=${HADOOP_DEFAULT_MAP_TASKS}
else
    defaultmaptasks=`expr ${maxtaskspernode} \* ${HADOOP_SLAVE_COUNT}`
fi

if [ "${HADOOP_DEFAULT_REDUCE_TASKS}X" != "X" ]
then
    defaultreducetasks=${HADOOP_DEFAULT_REDUCE_TASKS}
else
    defaultreducetasks=${HADOOP_SLAVE_COUNT}
fi

if [ "${HADOOP_MAX_MAP_TASKS}X" != "X" ]
then
    maxmaptasks=${HADOOP_MAX_MAP_TASKS}
else
    maxmaptasks=${maxtaskspernode}
fi

if [ "${HADOOP_MAX_REDUCE_TASKS}X" != "X" ]
then
    maxreducetasks=${HADOOP_MAX_REDUCE_TASKS}
else
    maxreducetasks=${maxtaskspernode}
fi

if [ "${HADOOP_HDFS_BLOCKSIZE}X" != "X" ]
then
    hdfsblocksize=${HADOOP_HDFS_BLOCKSIZE}
else
    # 64M is Hadoop default, widely considered bad choice, we'll use 128M as default
    hdfsblocksize=134217728
fi

if [ "${HADOOP_HDFS_REPLICATION}X" != "X" ]
then
    hdfsreplication=${HADOOP_HDFS_REPLICATION}
else
    hdfsreplication=3
fi

if [ "${HADOOP_RAWNETWORKFS_BLOCKSIZE}X" != "X" ] \
    && [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    rawnetworkfsblocksize=${HADOOP_RAWNETWORKFS_BLOCKSIZE}
else
    # 32M is Hadoop default for local
    rawnetworkfsblocksize=33554432
fi

if [ "${HADOOP_INTELLUSTRE_BLOCKSIZE}X" != "X" ]
then
    intellustreblocksize=${HADOOP_INTELLUSTRE_BLOCKSIZE}
else
    intellustreblocksize=536870912
fi

if [ "${HADOOP_INTELLUSTRE_STRIPESIZE}X" != "X" ]
then
    intellustrestripesize=${HADOOP_INTELLUSTRE_STRIPESIZE}
else
    intellustrestripesize=134217728
fi

if [ "${HADOOP_INTELLUSTRE_STRIPECOUNT}X" != "X" ]
then
    intellustrestripecount=${HADOOP_INTELLUSTRE_STRIPECOUNT}
else
    intellustrestripecount=-1
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ] && [ "${HADOOP_INTELLUSTRE_SHUFFLE}X" != "X" ]
then
    if [ "${HADOOP_INTELLUSTRE_SHUFFLE}" == "yes" ]
    then
	mapoutputcollectorclass="org.apache.hadoop.mapred.SharedFsPlugins\$MapOutputBuffer"
	reduceshuffleconsumerplugin="org.apache.hadoop.mapred.SharedFsPlugins\$Shuffle"
    else
	mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
	reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
    fi
else
    mapoutputcollectorclass="org.apache.hadoop.mapred.MapTask\$MapOutputBuffer"
    reduceshuffleconsumerplugin="org.apache.hadoop.mapreduce.task.reduce.Shuffle"
fi

if [ "${HADOOP_MAGPIENETWORKFS_BLOCKSIZE}X" != "X" ] \
    && [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    magpienetworkfsblocksize=${HADOOP_MAGPIENETWORKFS_BLOCKSIZE}
else
    # 32M is Hadoop default for local
    magpienetworkfsblocksize=33554432
fi

if [ "${HADOOP_DAEMON_HEAP_MAX}X" != "X" ]
then
    hadoopdaemonheapmax="${HADOOP_DAEMON_HEAP_MAX}"
else
    hadoopdaemonheapmax="1000"
fi 

if [ "${HADOOP_NAMENODE_DAEMON_HEAP_MAX}X" != "X" ]
then
    hadoopnamenodedaemonheapmax="${HADOOP_NAMENODE_DAEMON_HEAP_MAX}"
else
    hadoopnamenodedaemonheapmax="${hadoopdaemonheapmax}"
fi 

if [ "${HADOOP_COMPRESSION}X" != "X" ]
then
    if [ "${HADOOP_COMPRESSION}" == "yes" ]
    then
	compression=true
    else
	compression=false
    fi
else
    compression=false
fi

openfiles=`ulimit -n`
if [ "${openfiles}" != "unlimited" ]
then
    openfileshardlimit=`ulimit -H -n`

    # we estimate 4096 per 100 nodes, minimum 8192, max 65536.
    # Obviously depends on many factors such as core count, but it's a
    # reasonble and safe over-estimate calculated based on experience.
    openfilesslavecount=`expr ${HADOOP_SLAVE_COUNT} \/ 100`
    openfilescount=`expr ${openfilesslavecount} \* 4096`
    if [ "${openfilescount}" -lt "8192" ]
    then
	openfilescount=8192
    fi
    if [ "${openfilescount}" -gt "65536" ]
    then
	openfilescount=65536
    fi
    
    if [ "${openfileshardlimit}" != "unlimited" ]
    then
        if [ ${openfilescount} -gt ${openfileshardlimit} ]
        then
            openfilescount=${openfileshardlimit}
        fi
    fi
else
    openfilescount="unlimited"
fi

userprocesses=`ulimit -u`
if [ "${userprocesses}" != "unlimited" ]
then
    userprocesseshardlimit=`ulimit -H -u`

    # we estimate 2048 per 100 nodes, minimum 4096, max 32768.
    userprocessesslavecount=`expr ${HADOOP_SLAVE_COUNT} \/ 100`
    userprocessescount=`expr ${userprocessesslavecount} \* 2048`
    if [ "${userprocessescount}" -lt "4096" ]
    then
	userprocessescount=4096
    fi
    if [ "${userprocessescount}" -gt "32768" ]
    then
	userprocessescount=32768
    fi
    
    if [ "${userprocesseshardlimit}" != "unlimited" ]
    then
        if [ ${userprocessescount} -gt ${userprocesseshardlimit} ]
        then
            userprocessescount=${userprocesseshardlimit}
        fi
    fi
else
    userprocessescount="unlimited"
fi

#
# Setup Hadoop file system
#

pathcount=0

# sets hadooptmpdir and fsdefault
Magpie_calculate_hadoop_filesystem_paths ${hadoopnoderank}

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    # Assume if path doesn't exist must hdfsformat
    if [ -d "${hadooptmpdir}" ]
    then
	hdfsformat=0
    else
	hdfsformat=1
    fi

    IFSORIG=${IFS}
    IFS=","
    datanodedirtmp=""
    for hdfspath in ${HADOOP_HDFS_PATH}
    do
	if [ ! -d "${hdfspath}" ]
	then
	    mkdir -p ${hdfspath}
	    if [ $? -ne 0 ] ; then
		echo "mkdir failed making ${hdfspath}"
		exit 1
	    fi
	fi

	if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
	then
	    if [ "${datanodedirtmp}X" == "X" ]
	    then
		datanodedirtmp="${hdfspath}/dfs/data"
	    else
		datanodedirtmp="${datanodedirtmp},${hdfspath}/dfs/data"
	    fi
	elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
	then
	    if [ "${datanodedirtmp}X" == "X" ]
	    then
		datanodedirtmp="file://${hdfspath}/dfs/data"
	    else
		datanodedirtmp="${datanodedirtmp},file://${hdfspath}/dfs/data"
	    fi
	fi
	pathcount=$((pathcount+1))
    done
    IFS=${IFSORIG}
    datanodedir=`echo "${datanodedirtmp}" | sed "s/\\//\\\\\\\\\//g"`
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    # Assume if path doesn't exist must hdfsformat
    if [ -d "${hadooptmpdir}" ]
    then
	hdfsformat=0
    else
	hdfsformat=1
	mkdir -p ${hadooptmpdir}
	if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
	fi

	if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ]
	then
	    /usr/bin/lfs setstripe --size ${hdfsblocksize} --count 1 ${hadooptmpdir}
	fi
    fi
    
    if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
    then
	datanodedir="$\{hadoop.tmp.dir\}\/dfs\/data"
    elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
    then
	datanodedir="file:\/\/\$\{hadoop.tmp.dir\}\/dfs\/data"
    fi
    
    pathcount=1

    # Cleanup locks if requested
    if ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] && [ "${HADOOP_HDFSOVERLUSTRE_REMOVE_LOCKS}" == "yes" ]) \
	|| ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ] && [ "${HADOOP_HDFSOVERNETWORKFS_REMOVE_LOCKS}" == "yes" ])
    then
	for num in `seq 0 ${HADOOP_SLAVE_COUNT}`
	do
            if [ "${HADOOP_PER_JOB_HDFS_PATH}" == "yes" ]
            then
 	        lockfile="${HADOOP_HDFSOVERLUSTRE_PATH}/${MAGPIE_JOB_ID}/node-${num}/dfs/data/in_use.lock"
            else
 	        lockfile="${HADOOP_HDFSOVERLUSTRE_PATH}/node-${num}/dfs/data/in_use.lock"
            fi
	    rm -f ${lockfile}
	done
    fi
elif [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ]
then
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${HADOOP_RAWNETWORKFS_PATH}" ]
    then
        mkdir -p ${HADOOP_RAWNETWORKFS_PATH}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${HADOOP_RAWNETWORKFS_PATH}"
	    exit 1
        fi
    fi
	
    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
        fi
    fi
    
    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
then
    intellustrerootdir=`echo "${HADOOP_INTELLUSTRE_PATH}" | sed "s/\\//\\\\\\\\\//g"`
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`

    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
            echo "mkdir failed making ${hadooptmpdir}"
            exit 1
        fi
    fi

    pathcount=1
elif [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
then
    magpienetworkfsbase=`echo "${HADOOP_MAGPIENETWORKFS_PATH}" | sed "s/\\//\\\\\\\\\//g"`
    hadooptmpdirsubst=`echo "${hadooptmpdir}" | sed "s/\\//\\\\\\\\\//g"`
    fsdefaultsubst=`echo ${fsdefault} | sed "s/\\//\\\\\\\\\//g"`
    
    if [ ! -d "${HADOOP_MAGPIENETWORKFS_PATH}" ]
    then
        mkdir -p ${HADOOP_MAGPIENETWORKFS_PATH}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${HADOOP_MAGPIENETWORKFS_PATH}"
	    exit 1
        fi
    fi
    
    if [ ! -d "${hadooptmpdir}" ]
    then
        mkdir -p ${hadooptmpdir}
        if [ $? -ne 0 ] ; then
	    echo "mkdir failed making ${hadooptmpdir}"
	    exit 1
        fi
    fi

    pathcount=1
else
    echo "Illegal HADOOP_FILESYSTEM_MODE \"${HADOOP_FILESYSTEM_MODE}\" specified"
    exit 1
fi

if ([ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "rawnetworkfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]) \
    && [ "${HADOOP_LOCALSTORE}X" != "X" ]
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_LOCALSTORE}
    do
	localstoredirtmp=${localstorefile}
	mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
	yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
	
	if [ -d "${localstoredirtmp}" ]
	then
	    mkdir -p ${localstoredirtmp}
	    if [ $? -ne 0 ] ; then
		echo "mkdir failed making ${localstoredirtmp}"
		exit 1
	    fi
	fi

	if [ "${mapredlocalstoredir}X" == "X" ]
	then
	    mapredlocalstoredir="${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredirtmp}"
	else
	    mapredlocalstoredir="${mapredlocalstoredir},${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredir},${yarnlocalstoredirtmp}"
	fi
    done
    IFS=${IFSORIG}
elif [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    && [ ${pathcount} -gt 1 ] 
then
    IFSORIG=${IFS}
    IFS=","
    mapredlocalstoredir=""
    yarnlocalstoredir=""
    for localstorefile in ${HADOOP_HDFS_PATH}
    do
	localstoredirtmp=${localstorefile}
	mapredlocalstoredirtmp=`echo "${localstoredirtmp}/mapred/local" | sed "s/\\//\\\\\\\\\//g"`
	yarnlocalstoredirtmp=`echo "${localstoredirtmp}/yarn-nm" | sed "s/\\//\\\\\\\\\//g"`
	
	if [ "${mapredlocalstoredir}X" == "X" ]
	then
	    mapredlocalstoredir="${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredirtmp}"
	else
	    mapredlocalstoredir="${mapredlocalstoredir},${mapredlocalstoredirtmp}"
	    yarnlocalstoredir="${yarnlocalstoredir},${yarnlocalstoredirtmp}"
	fi
    done
    IFS=${IFSORIG}
else
    mapredlocalstoredir="\$\{hadoop.tmp.dir\}\/mapred\/local"
    yarnlocalstoredir="\$\{hadoop.tmp.dir\}\/yarn-nm"
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    # If HDFS would be stored in HDFS, so pick a better path
    yardappmapreduceamstagingdir="${HADOOP_LOCAL_DIR}/yarn/"
else
    yardappmapreduceamstagingdir="${hadooptmpdir}/yarn/"
fi

# Sets hadoopstoptimeout
Magpie_calculate_stop_timeouts

#
# Hadoop UDA Setup
#

if [ "${HADOOP_UDA_SETUP}" == "yes" ] \
    && [ "${HADOOP_SETUP_TYPE}"  == "MR2" ]
then
    if [ "${extrahadoopclasses}X" == "X" ]
    then
	extrahadoopclasses="${HADOOP_UDA_JAR}"
    else
	extrahadoopclasses="${extrahadoopclasses}:${HADOOP_UDA_JAR}"
    fi

    if [ "${HADOOP_UDA_LIBPATH}X" != "X" ]
    then
	if [ "${extrahadoopopts}X" == "X" ]
	then
	    extrahadoopopts="-Djava.library.path=${HADOOP_UDA_LIBPATH}"
	else
	    extrahadoopopts="${extrahadoopopts} -Djava.library.path=${HADOOP_UDA_LIBPATH}"
	fi

	if [ "${extrayarnlibrarypath}X" == "X" ]
	then
	    extrayarnlibrarypath="${HADOOP_UDA_LIBPATH}"
	else
	    extrayarnlibrarypath="${extrayarnlibrarypath}:${HADOOP_UDA_LIBPATH}"
	fi
    fi
fi

if [ "${HADOOP_UDA_RDMA_BUFFER_SIZE}X" != "X" ]
then
    rdmabufsize=${HADOOP_UDA_RDMA_BUFFER_SIZE}
else
    rdmabufsize=1024
fi

#
# Tachyon
#

if [ "${TACHYON_SETUP}" == "yes" ]
then
    tachyonjar="${TACHYON_HOME}/client/target/tachyon-client-${TACHYON_VERSION}-jar-with-dependencies.jar"

    if [ "${extrahadoopclasses}X" == "X" ]
    then
	extrahadoopclasses="${tachyonjar}"
    else
	extrahadoopclasses="${extrahadoopclasses}:${tachyonjar}"
    fi

    if [ "${TACHYON_ASYNCHRONOUS_WRITES}" == "yes" ]
    then
	if [ "${extrahadooptaskopts}X" == "X" ]
	then
	    extrahadooptaskopts="-Dtachyon.async.enabled=true -Dtachyon.user.file.writetype.default=ASYNC_THROUGH"
	else
	    extrahadooptaskopts="${extrahadooptaskopts} -Dtachyon.async.enabled=true -Dtachyon.user.file.writetype.default=ASYNC_THROUGH"
	fi
    fi
fi

#
# Get config files for setup
#

if [ "${HADOOP_CONF_FILES}X" == "X" ]
then
    hadoopconffiledir=${MAGPIE_SCRIPTS_HOME}/conf
else
    hadoopconffiledir=${HADOOP_CONF_FILES}
fi

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS1" ]
then
    pre_coresitexml=${hadoopconffiledir}/core-site-1.0.xml
    pre_mapredsitexml=${hadoopconffiledir}/mapred-site-1.0.xml
    pre_hadoopenvsh=${hadoopconffiledir}/hadoop-env-1.0.sh
    pre_hdfssitexml=${hadoopconffiledir}/hdfs-site-1.0.xml
    pre_log4jproperties=${hadoopconffiledir}/hadoop.log4j.properties
elif [ ${HADOOP_SETUP_TYPE}  == "MR2" ] || [ ${HADOOP_SETUP_TYPE} == "HDFS2" ]
then
    pre_coresitexml=${hadoopconffiledir}/core-site-2.0.xml
    pre_mapredsitexml=${hadoopconffiledir}/mapred-site-2.0.xml
    pre_hadoopenvsh=${hadoopconffiledir}/hadoop-env-2.0.sh
    pre_mapredenvsh=${hadoopconffiledir}/mapred-env-2.0.sh
    pre_yarnsitexml=${hadoopconffiledir}/yarn-site-2.0.xml
    pre_yarnenvsh=${hadoopconffiledir}/yarn-env-2.0.sh
    pre_hdfssitexml=${hadoopconffiledir}/hdfs-site-2.0.xml
    pre_log4jproperties=${hadoopconffiledir}/hadoop.log4j.properties

    if [ "${HADOOP_UDA_SETUP}" == "yes" ]
    then
	pre_mapredsitexml_uda=${hadoopconffiledir}/mapred-site-2.0-uda.xml
	pre_yarnsitexml_uda=${hadoopconffiledir}/yarn-site-2.0-uda.xml
    fi

    if [ "${HADOOP_FILESYSTEM_MODE}" == "intellustre" ]
    then
	pre_coresitexml_intellustre=${hadoopconffiledir}/core-site-2.0-intellustre.xml
    fi

    if [ "${HADOOP_FILESYSTEM_MODE}" == "magpienetworkfs" ]
    then
	pre_coresitexml_magpienetworkfs=${hadoopconffiledir}/core-site-2.0-magpienetworkfs.xml
    fi

    if [ "${TACHYON_SETUP}" == "yes" ]
    then
	pre_coresitexml_tachyon=${hadoopconffiledir}/core-site-2.0-tachyon.xml
    fi

    if [ "${HDFS_FEDERATION_NAMENODE_COUNT}" -gt 1 ]
    then
	pre_coresitexml_viewfsmount=${hadoopconffiledir}/core-site-2.0-viewfsmount.xml
    fi
else
    echo "Illegal HADOOP_SETUP_TYPE \"${HADOOP_SETUP_TYPE} \" specified"
    exit 1
fi

post_coresitexml=${HADOOP_CONF_DIR}/core-site.xml
post_mapredsitexml=${HADOOP_CONF_DIR}/mapred-site.xml
post_hadoopenvsh=${HADOOP_CONF_DIR}/hadoop-env.sh
post_mapredenvsh=${HADOOP_CONF_DIR}/mapred-env.sh
post_yarnsitexml=${HADOOP_CONF_DIR}/yarn-site.xml
post_yarnenvsh=${HADOOP_CONF_DIR}/yarn-env.sh
post_hdfssitexml=${HADOOP_CONF_DIR}/hdfs-site.xml
post_log4jproperties=${HADOOP_CONF_DIR}/log4j.properties

#
# Setup Hadoop configuration files and environment files
#

if [ ${HADOOP_SETUP_TYPE}  == "MR1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "MR2" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "HDFS1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "HDFS2" ]
then
    cp ${pre_coresitexml} ${post_coresitexml}

    if [ "${pre_coresitexml_intellustre}X" != "X" ]
    then
	sed -i -e "/@INTELLUSTRE@/{r ${pre_coresitexml_intellustre}" -e "d}" ${post_coresitexml}
    else
	sed -i -e "/@INTELLUSTRE@/,+1d" ${post_coresitexml}
    fi

    if [ "${pre_coresitexml_magpienetworkfs}X" != "X" ]
    then
	sed -i -e "/@MAGPIENETWORKFS@/{r ${pre_coresitexml_magpienetworkfs}" -e "d}" ${post_coresitexml}
    else
	sed -i -e "/@MAGPIENETWORKFS@/,+1d" ${post_coresitexml}
    fi

    if [ "${pre_coresitexml_tachyon}X" != "X" ]
    then
	sed -i -e "/@TACHYON@/{r ${pre_coresitexml_tachyon}" -e "d}" ${post_coresitexml}
    else
	sed -i -e "/@TACHYON@/,+1d" ${post_coresitexml}
    fi

    if [ "${pre_coresitexml_viewfsmount}X" != "X" ]
    then
	for i in `seq 1 ${HDFS_FEDERATION_NAMENODE_COUNT}`
	do
            j=1
            while true;
            do
                mountname="HDFS_FEDERATION_NAMENODE_${i}_MOUNT_${j}"
                eval mountvalue="\$$mountname"

                if [ "${mountvalue}X" == "X" ]
                then
                    break;
                fi
		
		sed -i -e "/@VIEWFSMOUNT@/{r ${pre_coresitexml_viewfsmount}" -e "d}" ${post_coresitexml}

	        # First namenode is based on master
		if [ "${i}" == "1" ]
		then
		    federationnamenodehost="${HADOOP_MASTER_NODE}"
		else
		    numline=`expr ${i} - 1`
		    federationnamenodehost=`sed -n "${numline}p" ${HADOOP_CONF_DIR}/namenode_hdfs_federation`
		fi

		mountvaluesubst=`echo "${mountvalue}" | sed "s/\\//\\\\\\\\\//g"`
		
		sed -i -e "s/VIEWFSPATH/${mountvaluesubst}/g" ${post_coresitexml}
		sed -i -e "s/VIEWFSNAMENODE/${federationnamenodehost}/g" ${post_coresitexml}
		sed -i -e "s/NAMESERVICEID/ns${i}/g" ${post_coresitexml}
		sed -i -e "s/DFSNAMENODERPCADDRESS/${HADOOP_HDFS_NAMENODE_ADDRESS}/g" ${post_coresitexml}

                j=$((j+1))
            done
	done

	sed -i -e "/@VIEWFSMOUNT@/,+1d" ${post_coresitexml}
	
    else
	sed -i -e "/@VIEWFSMOUNT@/,+1d" ${post_coresitexml}
    fi

    sed -i \
	-e "s/HADOOPTMPDIR/${hadooptmpdirsubst}/g" \
        -e "s/FSDEFAULT/${fsdefaultsubst}/g" \
	-e "s/IOBUFFERSIZE/${iobuffersize}/g" \
	-e "s/LOCALBLOCKSIZE/${rawnetworkfsblocksize}/g" \
	-e "s/INTELLUSTREROOTDIR/${intellustrerootdir}/g" \
	-e "s/INTELLUSTREBLOCKSIZE/${intellustreblocksize}/g" \
	-e "s/INTELLUSTRESTRIPESIZE/${intellustrestripesize}/g" \
	-e "s/INTELLUSTRESTRIPECOUNT/${intellustrestripecount}/g" \
	-e "s/MAGPIENETWORKFSBASEDIR/${magpienetworkfsbase}/g" \
	-e "s/MAGPIENETWORKFSBLOCKSIZE/${magpienetworkfsblocksize}/g" \
	-e "s/HADOOPDEFAULTUSER/${USER}/g" \
	${post_coresitexml}

    hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadooplogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophdfshomesubst=`echo "${HADOOP_HDFS_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrahadoopclassessubst=`echo "${extrahadoopclasses}" | sed "s/\\//\\\\\\\\\//g"`
    extrahadoopoptssubst=`echo "${extrahadoopopts}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_hadoopenvsh} ${post_hadoopenvsh}

    sed -i \
	-e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOP_NAMENODE_HEAP_MAX/${hadoopnamenodedaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
	-e "s/HADOOPLOGDIR/${hadooplogdirsubst}/g" \
	-e "s/HADOOPHOME/${hadoophomesubst}/g" \
	-e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
	-e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
	-e "s/HADOOPHDFSHOME/${hadoophdfshomesubst}/g" \
	-e "s/EXTRAHADOOPCLASSES/${extrahadoopclassessubst}/g" \
	-e "s/EXTRAHADOOPOPTS/${extrahadoopoptssubst}/g" \
	${post_hadoopenvsh}

    if [ "${MAGPIE_REMOTE_CMD:-ssh}" != "ssh" ]
    then
	echo "export HADOOP_SSH_CMD=\"${MAGPIE_REMOTE_CMD}\"" >> ${post_hadoopenvsh}
    fi
    if [ "${MAGPIE_REMOTE_CMD_OPTS}X" != "X" ]
    then
	echo "export HADOOP_SSH_OPTS=\"${MAGPIE_REMOTE_CMD_OPTS}\"" >> ${post_hadoopenvsh}
    fi

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
	cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${post_hadoopenvsh}
    else
	echo "ulimit -n ${openfilescount}" >> ${post_hadoopenvsh}
	echo "ulimit -u ${userprocessescount}" >> ${post_hadoopenvsh}
    fi

    cp ${pre_log4jproperties} ${post_log4jproperties}
fi

if [ ${HADOOP_SETUP_TYPE} == "MR1" ] \
    || [ ${HADOOP_SETUP_TYPE}  == "MR2" ]
then
    if [ "${HADOOP_UDA_SETUP}" == "yes" ]
    then
        #
        # A few settings depend on version
        #
	if echo ${HADOOP_VERSION} | grep -q -E "2\.[0-1]\.[0-9]"
	then 
	    shuffleproviderservices="uda.shuffle"
	else
	    shuffleproviderservices="uda_shuffle"
	fi
	reduceshuffleconsumerplugin="com.mellanox.hadoop.mapred.UdaShuffleConsumerPlugin"
    fi

    yardappmapreduceamstagingdirsubst=`echo "${yardappmapreduceamstagingdir}" | sed "s/\\//\\\\\\\\\//g"`
    extrahadooptaskoptssubst=`echo "${extrahadooptaskopts}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_mapredsitexml} ${post_mapredsitexml}

    if [ "${pre_mapredsitexml_uda}X" != "X" ]
    then
	sed -i -e "/@UDA@/{r ${pre_mapredsitexml_uda}" -e "d}" ${post_mapredsitexml}
    else
	sed -i -e "/@UDA@/,+1d" ${post_mapredsitexml}
    fi

    # Space w/ HADOOPEXTRATASKJAVAOPTS substitution is intentional
    sed -i \
	-e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/MRPRALLELCOPIES/${parallelcopies}/g" \
	-e "s/JOBTRACKERHANDLERCOUNT/${jobtrackerhandlercount}/g" \
	-e "s/MRSLOWSTART/${mapredslowstart}/g" \
	-e "s/ALLCHILDHEAPSIZE/${allchildheapsize}/g" \
	-e "s/MAPCHILDHEAPSIZE/${mapchildheapsize}/g" \
	-e "s/MAPCONTAINERMB/${mapcontainermb}/g" \
	-e "s/REDUCECHILDHEAPSIZE/${reducechildheapsize}/g" \
	-e "s/REDUCECONTAINERMB/${reducecontainermb}/g" \
	-e "s/DEFAULTMAPTASKS/${defaultmaptasks}/g" \
	-e "s/DEFAULTREDUCETASKS/${defaultreducetasks}/" \
	-e "s/MAXMAPTASKS/${maxmaptasks}/g" \
	-e "s/MAXREDUCETASKS/${maxreducetasks}/g" \
	-e "s/LOCALSTOREDIR/${mapredlocalstoredir}/g" \
	-e "s/IOSORTFACTOR/${iosortfactor}/g" \
	-e "s/IOSORTMB/${iosortmb}/g" \
	-e "s/HADOOPCOMPRESSION/${compression}/g" \
	-e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
	-e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
	-e "s/SUBMITFILEREPLICATION/${submitfilereplication}/g" \
    	-e "s/MAPOUTPUTCOLLECTORCLASS/${mapoutputcollectorclass}/g" \
    	-e "s/REDUCESHUFFLECONSUMERPLUGIN/${reduceshuffleconsumerplugin}/g" \
	-e "s/SHUFFLEPROVIDERSERVICES/${shuffleproviderservices}/g" \
	-e "s/RDMABUFSIZE/${rdmabufsize}/g" \
	-e "s/HADOOPJOBHISTORYADDRESS/${HADOOP_JOBHISTORYSERVER_ADDRESS}/g" \
	-e "s/HADOOPJOBHISTORYWEBAPPADDRESS/${HADOOP_JOBHISTORYSERVER_WEBAPP_ADDRESS}/g" \
	-e "s/MAPREDJOBTRACKERADDRESS/${MAPRED_JOB_TRACKER_ADDRESS}/g" \
	-e "s/MAPREDJOBTRACKERHTTPADDRESS/${MAPRED_JOB_TRACKER_HTTPADDRESS}/g" \
	-e "s/YARNAPPMAPREDUCEAMSTAGINGDIR/${yardappmapreduceamstagingdirsubst}/g" \
	-e "s/HADOOPEXTRATASKJAVAOPTS/ ${extrahadooptaskoptssubst}/g" \
	${post_mapredsitexml}
fi

if [ ${HADOOP_SETUP_TYPE}  == "MR2" ]
then
    #
    # A few settings depend on version
    #
    if echo ${HADOOP_VERSION} | grep -q -E "2\.[0-1]\.[0-9]"
    then 
	if [ "${HADOOP_UDA_SETUP}" == "yes" ]
	then
	    yarnauxservices="mapreduce.shuffle,uda.shuffle"
	    yarnauxudashuffle="yarn.nodemanager.aux-services.uda.shuffle.class"
	else
	    yarnauxservices="mapreduce.shuffle"
	fi
	yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce.shuffle.class"
    else
	if [ "${HADOOP_UDA_SETUP}" == "yes" ]
	then
	    yarnauxservices="mapreduce_shuffle,uda_shuffle"
	    yarnauxudashuffle="yarn.nodemanager.aux-services.uda_shuffle.class"
	else
	    yarnauxservices="mapreduce_shuffle"
	fi
	yarnauxmapreduceshuffle="yarn.nodemanager.aux-services.mapreduce_shuffle.class"
    fi

    cp ${pre_yarnsitexml} ${post_yarnsitexml}

    if [ "${pre_yarnsitexml_uda}X" != "X" ]
    then
	sed -i -e "/@UDA@/{r ${pre_yarnsitexml_uda}" -e "d}" ${post_yarnsitexml}
    else
	sed -i -e "/@UDA@/,+1d" ${post_yarnsitexml}
    fi

    sed -i \
	-e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/YARNMINCONTAINER/${yarnmincontainer}/g" \
	-e "s/YARNMAXCONTAINER/${yarnmaxcontainer}/g" \
	-e "s/YARNRESOURCEMEMORY/${yarnresourcememory}/g" \
	-e "s/LOCALSTOREDIR/${yarnlocalstoredir}/g" \
	-e "s/YARNAUXSERVICES/${yarnauxservices}/g" \
	-e "s/YARNAUXMAPREDUCESHUFFLE/${yarnauxmapreduceshuffle}/g" \
	-e "s/YARNAUXUDASHUFFLE/${yarnauxudashuffle}/g" \
	-e "s/YARNRESOURCEMANAGERADDRESS/${YARN_RESOURCEMANAGER_ADDRESS}/g" \
	-e "s/YARNRESOURCEMANAGERSCHEDULERADDRESS/${YARN_RESOURCEMANAGER_SCHEDULER_ADDRESS}/g" \
	-e "s/YARNRESOURCEMANAGERWEBAPPADDRESS/${YARN_RESOURCEMANAGER_WEBAPP_ADDRESS}/g" \
	-e "s/YARNRESOURCEMANAGERWEBAPPHTTPSADDRESS/${YARN_RESOURCEMANAGER_WEBAPP_HTTPS_ADDRESS}/g" \
	-e "s/YARNRESOURCEMANAGERRESOURCETRACKERADDRESS/${YARN_RESOURCEMANAGER_RESOURCETRACKER_ADDRESS}/g" \
	-e "s/YARNRESOURCEMANAGERADMINADDRESS/${YARN_RESOURCEMANAGER_ADMIN_ADDRESS}/g" \
	-e "s/YARNLOCALIZERADDRESS/${YARN_NODEMANAGER_LOCALIZER_ADDRESS}/g" \
	-e "s/YARNNODEMANAGERWEBAPPADDRESS/${YARN_NODEMANAGER_WEBAPP_ADDRESS}/g" \
	-e "s/YARNDEFAULTUSER/${USER}/g" \
	${post_yarnsitexml}
    
    hadoopmapredlogdirsubst=`echo "${HADOOP_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopconfdirsubst=`echo "${HADOOP_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    hadoophomesubst=`echo "${HADOOP_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopcommonhomesubst=`echo "${HADOOP_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopmapredhomesubst=`echo "${HADOOP_MAPRED_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_mapredenvsh} ${post_mapredenvsh}

    sed -i \
	-e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/HADOOPMAPREDLOGDIR/${hadoopmapredlogdirsubst}/g" \
	-e "s/HADOOPCONFDIR/${hadoopconfdirsubst}/g" \
	-e "s/HADOOPHOME/${hadoophomesubst}/g" \
	-e "s/HADOOPCOMMONHOME/${hadoopcommonhomesubst}/g" \
	-e "s/HADOOPMAPREDHOME/${hadoopmapredhomesubst}/g" \
	${post_mapredenvsh}

    yarnconfdirsubst=`echo "${YARN_CONF_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarnlogdirsubst=`echo "${YARN_LOG_DIR}" | sed "s/\\//\\\\\\\\\//g"`
    yarncommonhomesubst=`echo "${YARN_COMMON_HOME}" | sed "s/\\//\\\\\\\\\//g"`
    hadoopyarnhomesubst=`echo "${HADOOP_YARN_HOME}" | sed "s/\\//\\\\\\\\\//g"`

    extrayarnlibrarypathsubst=`echo "${extrayarnlibrarypath}" | sed "s/\\//\\\\\\\\\//g"`

    cp ${pre_yarnenvsh} ${post_yarnenvsh}

    sed -i \
	-e "s/HADOOP_JAVA_HOME/${javahomesubst}/g" \
	-e "s/HADOOP_DAEMON_HEAP_MAX/${hadoopdaemonheapmax}/g" \
	-e "s/HADOOPTIMEOUTSECONDS/${hadoopstoptimeout}/g" \
	-e "s/YARNUSERNAME/${HADOOP_YARN_USER}/g" \
	-e "s/YARNCONFDIR/${yarnconfdirsubst}/g" \
	-e "s/YARNLOGDIR/${yarnlogdirsubst}/g" \
	-e "s/YARNCOMMONHOME/${yarncommonhomesubst}/g" \
	-e "s/HADOOPYARNHOME/${hadoopyarnhomesubst}/g" \
	-e "s/EXTRAYARNLIBRARYPATH/${extrayarnlibrarypathsubst}/g" \
	${post_yarnenvsh}

    if [ "${MAGPIE_REMOTE_CMD:-ssh}" != "ssh" ]
    then
	echo "export YARN_SSH_CMD=\"$MAGPIE_REMOTE_CMD\"" >> ${post_yarnenvsh}
    fi
    if [ "${MAGPIE_REMOTE_CMD_OPTS}X" != "X" ]
    then
	echo "export YARN_SSH_OPTS=\"${MAGPIE_REMOTE_CMD_OPTS}\"" >> ${post_yarnenvsh}
    fi

    if [ "${HADOOP_ENVIRONMENT_EXTRA_PATH}X" != "X" ] && [ -f ${HADOOP_ENVIRONMENT_EXTRA_PATH} ]
    then
	cat ${HADOOP_ENVIRONMENT_EXTRA_PATH} >> ${post_yarnenvsh}
    else
	echo "ulimit -n ${openfilescount}" >> ${post_yarnenvsh}
	echo "ulimit -u ${userprocessescount}" >> ${post_yarnenvsh}
    fi
fi

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    cp ${pre_hdfssitexml} ${post_hdfssitexml}

    if [ "${HDFS_FEDERATION_NAMENODE_COUNT}" -gt 1 ]
    then
	pre_hdfssitexml_namenode=${hadoopconffiledir}/hdfs-site-2.0-hdfs-federation.xml
    else
	pre_hdfssitexml_namenode=${hadoopconffiledir}/hdfs-site-2.0-hdfs-namenode.xml
    fi

    sed -i -e "/@NAMENODE@/{r ${pre_hdfssitexml_namenode}" -e "d}" ${post_hdfssitexml}

    if [ "${HDFS_FEDERATION_NAMENODE_COUNT}" -gt 1 ]
    then
	nameservices="ns1"
	for i in `seq 2 ${HDFS_FEDERATION_NAMENODE_COUNT}`
        do
	    nameservices="${nameservices},ns${i}"
	done

	sed -i -e "s/NAMESERVICES/${nameservices}/g" ${post_hdfssitexml}

	pre_hdfssitexml_namenodeinfo=${hadoopconffiledir}/hdfs-site-2.0-hdfs-federation-namenode.xml

	# Now fill each additional namenode
	for i in `seq 1 ${HDFS_FEDERATION_NAMENODE_COUNT}`
	do
	    sed -i -e "/@NAMENODEINFO@/{r ${pre_hdfssitexml_namenodeinfo}" -e "d}" ${post_hdfssitexml}

	    sed -i -e "s/NAMESERVICEID/ns${i}/g" ${post_hdfssitexml}

	    # First namenode is based on master
	    if [ "${i}" == "1" ]
	    then
		federationnamenodehost="${HADOOP_MASTER_NODE}"
	    else
		numline=`expr ${i} - 1`
		federationnamenodehost=`sed -n "${numline}p" ${HADOOP_CONF_DIR}/namenode_hdfs_federation`
	    fi
	    
	    sed -i -e "s/FEDERATIONNAMENODEHOST/${federationnamenodehost}/g" ${post_hdfssitexml}
	done

	sed -i -e "/@NAMENODEINFO@/,+1d" ${post_hdfssitexml}
    fi

    HADOOP_THIS_HOST=`hostname`

    sed -i \
	-e "s/HADOOP_MASTER_HOST/${HADOOP_MASTER_NODE}/g" \
	-e "s/HADOOP_HOST/${HADOOP_THIS_HOST}/g" \
	-e "s/HDFSBLOCKSIZE/${hdfsblocksize}/g" \
	-e "s/HDFSREPLICATION/${hdfsreplication}/g" \
	-e "s/HDFSNAMENODEHANDLERCLOUNT/${namenodehandlercount}/g" \
	-e "s/HDFSDATANODEHANDLERCLOUNT/${datanodehandlercount}/g" \
	-e "s/IOBUFFERSIZE/${iobuffersize}/g" \
	-e "s/HADOOPHOSTSINCLUDEFILENAME/${hostsincludefilesubst}/g" \
	-e "s/HADOOPHOSTSEXCLUDEFILENAME/${hostsexcludefilesubst}/g" \
	-e "s/DFSDATANODEDIR/${datanodedir}/g" \
	-e "s/DFSNAMENODERPCADDRESS/${HADOOP_HDFS_NAMENODE_ADDRESS}/g" \
	-e "s/DFSNAMENODESECONDARYHTTPADDRESS/${HADOOP_HDFS_NAMENODE_SECONDARY_HTTP_ADDRESS}/g" \
	-e "s/DFSNAMENODESECONDARYHTTPSADDRESS/${HADOOP_HDFS_NAMENODE_SECONDARY_HTTPS_ADDRESS}/g" \
	-e "s/DFSNAMENODEHTTPADDRESS/${HADOOP_HDFS_NAMENODE_HTTPADDRESS}/g" \
	-e "s/DFSDATANODEADDRESS/${HADOOP_HDFS_DATANODE_ADDRESS}/g" \
	-e "s/DFSDATANODEHTTPADDRESS/${HADOOP_HDFS_DATANODE_HTTPADDRESS}/g" \
	-e "s/DFSDATANODEIPCADDRESS/${HADOOP_HDFS_DATANODE_IPCADDRESS}/g" \
	-e "s/DFSBACKUPADDRESS/${HADOOP_HDFS_NAMENODE_BACKUP_ADDRESS}/g" \
	-e "s/DFSBACKUPHTTPADDRESS/${HADOOP_HDFS_NAMENODE_BACKUP_HTTP_ADDRESS}/g" \
	-e "s/DFSPERMISSIONSSUPERUSERGROUP/${USER}/g" \
	${post_hdfssitexml}
fi

#
# Perform HDFS format if necessary
#

if [ "${HADOOP_FILESYSTEM_MODE}" == "hdfs" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsoverlustre" ] \
    || [ "${HADOOP_FILESYSTEM_MODE}" == "hdfsovernetworkfs" ]
then
    if [ "${hdfsformat}" -eq "1" ]
    then
	if [ "${HDFS_FEDERATION_NAMENODE_COUNT}" -gt 1 ]
	then
	    if Magpie_am_I_a_hadoop_namenode
	    then
		echo "*******************************************************"
		echo "* Formatting HDFS Namenode on Namenode ${MAGPIE_CLUSTER_NODERANK}"
		echo "*******************************************************"
		cd $HADOOP_HOME
		echo 'Y' | ${hadoopcmdprefix}/hdfs namenode -format -clusterId "Magpie${HADOOP_FILESYSTEM_MODE}"
	    else
	        # If this is the first time running, make everyone else wait
	        # until the format is complete
		sleep 30
	    fi
	else
            # Only master will format the node
	    if Magpie_am_I_master
	    then
		echo "*******************************************************"
		echo "* Formatting HDFS Namenode"
		echo "*******************************************************"
		cd $HADOOP_HOME
		echo 'Y' | ${hadoopcmdprefix}/hadoop namenode -format
	    else
	        # If this is the first time running, make everyone else wait
	        # until the format is complete
		sleep 30
	    fi
	fi
    fi
fi

exit 0
