diff --git a/tpcds-setup-beeline.sh b/tpcds-setup-beeline.sh
new file mode 100755
index 0000000..d1ca8ba
--- /dev/null
+++ b/tpcds-setup-beeline.sh
@@ -0,0 +1,129 @@
+#!/bin/bash
+
+function usage {
+	echo "Usage: tpcds-setup.sh scale_factor [temp_directory]"
+	exit 1
+}
+
+function runcommand {
+	if [ "X$DEBUG_SCRIPT" != "X" ]; then
+		$1
+	else
+		$1 2>/dev/null
+	fi
+}
+
+if [ ! -f tpcds-gen/target/tpcds-gen-1.0-SNAPSHOT.jar ]; then
+	echo "Please build the data generator with ./tpcds-build.sh first"
+	exit 1
+fi
+which hive > /dev/null 2>&1
+if [ $? -ne 0 ]; then
+	echo "Script must be run where Hive is installed"
+	exit 1
+fi
+
+# Tables in the TPC-DS schema.
+DIMS="date_dim time_dim item customer customer_demographics household_demographics customer_address store promotion warehouse ship_mode reason income_band call_center web_page catalog_page web_site"
+FACTS="store_sales store_returns web_sales web_returns catalog_sales catalog_returns inventory"
+
+# Get the parameters.
+SERVER=$1
+PORT=$2
+SCALE=$3
+DIR=$4
+
+if [ "X$BUCKET_DATA" != "X" ]; then
+	BUCKETS=13
+	RETURN_BUCKETS=13
+else
+	BUCKETS=1
+	RETURN_BUCKETS=1
+fi
+if [ "X$DEBUG_SCRIPT" != "X" ]; then
+	set -x
+fi
+
+# Sanity checking.
+if [ X"$SERVER" = "X" ]; then
+        echo "Server argument must be supplied"
+        exit 1
+fi
+if [ X"$PORT" = "X" ]; then
+        echo "Port argument must be supplied"
+        exit 1
+fi
+if [ X"$SCALE" = "X" ]; then
+	usage
+fi
+if [ X"$DIR" = "X" ]; then
+	DIR=/tmp/tpcds-generate
+fi
+if [ $SCALE -eq 1 ]; then
+	echo "Scale factor must be greater than 1"
+	exit 1
+fi
+
+# Do the actual data load.
+hdfs dfs -mkdir -p ${DIR}
+hdfs dfs -ls ${DIR}/${SCALE} > /dev/null
+if [ $? -ne 0 ]; then
+	echo "Generating data at scale factor $SCALE."
+	(cd tpcds-gen; hadoop jar target/*.jar -d ${DIR}/${SCALE}/ -s ${SCALE})
+fi
+hdfs dfs -ls ${DIR}/${SCALE} > /dev/null
+if [ $? -ne 0 ]; then
+	echo "Data generation failed, exiting."
+	exit 1
+fi
+echo "TPC-DS text data generation complete."
+
+# Create the text/flat tables as external tables. These will be later be converted to ORCFile.
+echo "Loading text data into external tables."
+runcommand "beeline -u jdbc:hive2://${SERVER}:${PORT} -i settings/load-flat.sql --hivevar DB=tpcds_text_${SCALE} --hivevar LOCATION=${DIR}/${SCALE} -f ddl-tpcds/text/alltables.sql"
+
+# Create the partitioned and bucketed tables.
+if [ "X$FORMAT" = "X" ]; then
+	FORMAT=orc
+fi
+
+LOAD_FILE="load_${FORMAT}_${SCALE}.mk"
+SILENCE="2> /dev/null 1> /dev/null" 
+if [ "X$DEBUG_SCRIPT" != "X" ]; then
+	SILENCE=""
+fi
+
+echo -e "all: ${DIMS} ${FACTS}" > $LOAD_FILE
+
+i=1
+total=24
+DATABASE=tpcds_bin_partitioned_${FORMAT}_${SCALE}
+MAX_REDUCERS=2500 # maximum number of useful reducers for any scale 
+REDUCERS=$((test ${SCALE} -gt ${MAX_REDUCERS} && echo ${MAX_REDUCERS}) || echo ${SCALE})
+
+# Populate the smaller tables.
+for t in ${DIMS}
+do
+	COMMAND="beeline -u jdbc:hive2://${SERVER}:${PORT} -i settings/load-partitioned.sql -f ddl-tpcds/bin_partitioned/${t}.sql \
+	    --hivevar DB=${DATABASE} --hivevar SOURCE=tpcds_text_${SCALE} \
+            --hivevar SCALE=${SCALE} \
+	    --hivevar REDUCERS=${REDUCERS} \
+	    --hivevar FILE=${FORMAT}"
+	echo -e "${t}:\n\t@$COMMAND $SILENCE && echo 'Optimizing table $t ($i/$total).'" >> $LOAD_FILE
+	i=`expr $i + 1`
+done
+
+for t in ${FACTS}
+do
+	COMMAND="beeline -u jdbc:hive2://${SERVER}:${PORT} -i settings/load-partitioned.sql -f ddl-tpcds/bin_partitioned/${t}.sql \
+	    --hivevar DB=${DATABASE} \
+            --hivevar SCALE=${SCALE} \
+	    --hivevar SOURCE=tpcds_text_${SCALE} --hivevar BUCKETS=${BUCKETS} \
+	    --hivevar RETURN_BUCKETS=${RETURN_BUCKETS} --hivevar REDUCERS=${REDUCERS} -d FILE=${FORMAT}"
+	echo -e "${t}:\n\t@$COMMAND $SILENCE && echo 'Optimizing table $t ($i/$total).'" >> $LOAD_FILE
+	i=`expr $i + 1`
+done
+
+make -j 2 -f $LOAD_FILE
+
+echo "Data loaded into database ${DATABASE}."
diff --git a/tpch-setup-beeline.sh b/tpch-setup-beeline.sh
new file mode 100755
index 0000000..00fee46
--- /dev/null
+++ b/tpch-setup-beeline.sh
@@ -0,0 +1,112 @@
+#!/bin/bash
+
+function usage {
+	echo "Usage: tpch-setup.sh hive_server hive_port scale_factor [temp_directory]"
+	exit 1
+}
+
+function runcommand {
+	if [ "X$DEBUG_SCRIPT" != "X" ]; then
+		$1
+	else
+		$1 2>/dev/null
+	fi
+}
+
+if [ ! -f tpch-gen/target/tpch-gen-1.0-SNAPSHOT.jar ]; then
+	echo "Please build the data generator with ./tpch-build.sh first"
+	exit 1
+fi
+which hive > /dev/null 2>&1
+if [ $? -ne 0 ]; then
+	echo "Script must be run where Hive is installed"
+	exit 1
+fi
+
+# Tables in the TPC-H schema.
+TABLES="part partsupp supplier customer orders lineitem nation region"
+
+# Get the parameters.
+SERVER=$1
+PORT=$2
+SCALE=$3
+DIR=$4
+
+BUCKETS=13
+if [ "X$DEBUG_SCRIPT" != "X" ]; then
+	set -x
+fi
+
+# Sanity checking.
+
+if [ X"$SERVER" = "X" ]; then
+        echo "Server argument must be supplied"
+        exit 1
+fi
+if [ X"$PORT" = "X" ]; then
+        echo "Port argument must be supplied"
+        exit 1
+fi
+if [ X"$SCALE" = "X" ]; then
+	usage
+fi
+if [ X"$DIR" = "X" ]; then
+	DIR=/tmp/tpch-generate
+fi
+if [ $SCALE -eq 1 ]; then
+	echo "Scale factor must be greater than 1"
+	exit 1
+fi
+
+# Do the actual data load.
+hdfs dfs -mkdir -p ${DIR}
+hdfs dfs -ls ${DIR}/${SCALE}/lineitem > /dev/null
+if [ $? -ne 0 ]; then
+	echo "Generating data at scale factor $SCALE."
+	(cd tpch-gen; hadoop jar target/*.jar -d ${DIR}/${SCALE}/ -s ${SCALE})
+fi
+hdfs dfs -ls ${DIR}/${SCALE}/lineitem > /dev/null
+if [ $? -ne 0 ]; then
+	echo "Data generation failed, exiting."
+	exit 1
+fi
+echo "TPC-H text data generation complete."
+
+# Create the text/flat tables as external tables. These will be later be converted to ORCFile.
+echo "Loading text data into external tables."
+#runcommand "hive -i settings/load-flat.sql -f ddl-tpch/bin_flat/alltables.sql -d DB=tpch_text_${SCALE} -d LOCATION=${DIR}/${SCALE}"
+runcommand "beeline -u jdbc:hive2://${SERVER}:${PORT} -i settings/load-flat.sql --hivevar DB=tpch_text_${SCALE} --hivevar LOCATION=${DIR}/${SCALE} -f ddl-tpch/bin_flat/alltables.sql"
+
+# Create the optimized tables.
+i=1
+total=8
+
+if test $SCALE -le 1000; then 
+	SCHEMA_TYPE=flat
+else
+	SCHEMA_TYPE=partitioned
+fi
+
+DATABASE=tpch_${SCHEMA_TYPE}_orc_${SCALE}
+MAX_REDUCERS=2600 # ~7 years of data
+REDUCERS=$((test ${SCALE} -gt ${MAX_REDUCERS} && echo ${MAX_REDUCERS}) || echo ${SCALE})
+
+for t in ${TABLES}
+do
+	echo "Optimizing table $t ($i/$total)."
+	COMMAND="beeline -u jdbc:hive2://${SERVER}:${PORT} -i settings/load-${SCHEMA_TYPE}.sql \
+		-f ddl-tpch/bin_${SCHEMA_TYPE}/${t}.sql --hivevar DB=${DATABASE} \
+		--hivevar SOURCE=tpch_text_${SCALE} --hivevar BUCKETS=${BUCKETS} \
+		--hivevar SCALE=${SCALE} --hivevar REDUCERS=${REDUCERS} --hivevar FILE=orc"
+
+	runcommand "$COMMAND"
+	if [ $? -ne 0 ]; then
+		echo "Command failed (returned ${?} ) , try 'export DEBUG_SCRIPT=ON' and re-running"
+		exit 1
+	fi
+	i=`expr $i + 1`
+done
+
+beeline -u jdbc:hive2://${SERVER}:${PORT}/${DATABASE} -i settings/load-${SCHEMA_TYPE}.sql -f ddl-tpch/bin_${SCHEMA_TYPE}/analyze.sql; 
+
+echo "Data loaded into database ${DATABASE}."
