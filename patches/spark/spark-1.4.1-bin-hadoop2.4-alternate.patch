diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/slaves.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/slaves.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh	2016-08-15 15:19:25.937451000 -0700
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -79,19 +81,19 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
-
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
 
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `echo "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
   if [ -n "${SPARK_SSH_FOREGROUND}" ]; then
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /"
   else
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /" &
   fi
   if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh	2016-08-15 15:19:25.976412000 -0700
@@ -137,7 +137,8 @@ run_command() {
 
   if [ "$SPARK_MASTER" != "" ]; then
     echo rsync from "$SPARK_MASTER"
-    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$SPARK_HOME"
+    RSH_CMD=${SPARK_SSH_CMD:-ssh}
+    rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "$SPARK_HOME"
   fi
 
   spark_rotate_log "$log"
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh	2016-08-15 15:19:25.978412000 -0700
@@ -30,6 +30,24 @@ fi
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
-exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" "$@"
+exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" --config $SPARK_CONF_DIR "$@"
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/start-slave.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/start-slave.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh	2016-08-15 15:19:25.981408000 -0700
@@ -42,6 +42,24 @@ fi
 sbin="`dirname "$0"`"
 sbin="`cd "$sbin"; pwd`"
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
 . "$SPARK_PREFIX/bin/load-spark-env.sh"
@@ -71,8 +89,14 @@ function start_instance {
   fi
   WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))
 
-  "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker $WORKER_NUM \
-     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  if [ "${SPARK_CONF_DIR}X" != "X" ]
+  then
+      "$sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker $WORKER_NUM \
+	  --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  else
+      "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker $WORKER_NUM \
+	  --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  fi
 }
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/start-slaves.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/start-slaves.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh	2016-08-15 15:19:25.986402000 -0700
@@ -52,11 +52,11 @@ if [ "$SPARK_MASTER_IP" = "" ]; then
 fi
 
 if [ "$START_TACHYON" == "true" ]; then
-  "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin"/../tachyon/bin/tachyon bootstrap-conf "$SPARK_MASTER_IP"
+  "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin"/../tachyon/bin/tachyon bootstrap-conf "$SPARK_MASTER_IP"
 
   # set -t so we can call sudo
-  SPARK_SSH_OPTS="-o StrictHostKeyChecking=no -t" "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/../tachyon/bin/tachyon-start.sh" worker SudoMount \; sleep 1
+  SPARK_SSH_OPTS="-o StrictHostKeyChecking=no -t" "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/../tachyon/bin/tachyon-start.sh" worker SudoMount \; sleep 1
 fi
 
 # Launch the slaves
-"$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
+"$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR "spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT"
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/stop-slave.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slave.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/stop-slave.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slave.sh	2016-08-15 15:19:25.988405000 -0700
@@ -32,6 +32,23 @@ sbin="`cd "$sbin"; pwd`"
 
 . "$sbin/spark-config.sh"
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir="$1"
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR="$conf_dir"
+  fi
+  shift
+fi
+
 . "$SPARK_PREFIX/bin/load-spark-env.sh"
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
diff -pruN spark-1.4.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh
--- spark-1.4.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh	2015-07-08 15:59:42.000000000 -0700
+++ spark-1.4.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh	2016-08-15 15:19:25.991397000 -0700
@@ -26,7 +26,7 @@ sbin="`cd "$sbin"; pwd`"
 
 # do before the below calls as they exec
 if [ -e "$sbin"/../tachyon/bin/tachyon ]; then
-  "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin"/../tachyon/bin/tachyon killAll tachyon.worker.Worker
+  "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin"/../tachyon/bin/tachyon killAll tachyon.worker.Worker
 fi
 
-"$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin"/stop-slave.sh
+"$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin"/stop-slave.sh --config $SPARK_CONF_DIR
