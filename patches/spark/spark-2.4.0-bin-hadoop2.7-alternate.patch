diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/slaves.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/slaves.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/slaves.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/slaves.sh	2018-11-12 10:38:28.643421000 -0800
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -80,19 +82,19 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
-
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
 
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `echo "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
   if [ -n "${SPARK_SSH_FOREGROUND}" ]; then
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /"
   else
-    ssh $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
+    $RSH_CMD $SPARK_SSH_OPTS "$slave" $"${@// /\\ }" \
       2>&1 | sed "s/^/$slave: /" &
   fi
   if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/spark-daemon.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/spark-daemon.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/spark-daemon.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/spark-daemon.sh	2018-11-12 10:38:28.658394000 -0800
@@ -167,7 +167,8 @@ run_command() {
 
   if [ "$SPARK_MASTER" != "" ]; then
     echo rsync from "$SPARK_MASTER"
-    rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "${SPARK_HOME}"
+    RSH_CMD=${SPARK_SSH_CMD:-ssh}
+    rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' "$SPARK_MASTER/" "${SPARK_HOME}"
   fi
 
   spark_rotate_log "$log"
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/spark-daemons.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/spark-daemons.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/spark-daemons.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/spark-daemons.sh	2018-11-12 10:38:28.666391000 -0800
@@ -31,6 +31,24 @@ if [ -z "${SPARK_HOME}" ]; then
   export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
-exec "${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/spark-daemon.sh" "$@"
+exec "${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/spark-daemon.sh" "$@"
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/start-slave.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/start-slave.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/start-slave.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/start-slave.sh	2018-11-12 10:38:28.673456000 -0800
@@ -49,6 +49,24 @@ if [[ $# -lt 1 ]] || [[ "$@" = *--help ]
   exit 1
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
@@ -78,8 +96,14 @@ function start_instance {
   fi
   WEBUI_PORT=$(( $SPARK_WORKER_WEBUI_PORT + $WORKER_NUM - 1 ))
 
-  "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \
-     --webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  if [ "${SPARK_CONF_DIR}X" != "X" ]
+  then
+    "${SPARK_HOME}/sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start $CLASS $WORKER_NUM \
+	--webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  else
+    "${SPARK_HOME}/sbin"/spark-daemon.sh start $CLASS $WORKER_NUM \
+	--webui-port "$WEBUI_PORT" $PORT_FLAG $PORT_NUM $MASTER "$@"
+  fi
 }
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/start-slaves.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/start-slaves.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/start-slaves.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/start-slaves.sh	2018-11-12 10:38:28.681406000 -0800
@@ -43,4 +43,4 @@ if [ "$SPARK_MASTER_HOST" = "" ]; then
 fi
 
 # Launch the slaves
-"${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/start-slave.sh" "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
+"${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin/start-slave.sh" --config $SPARK_CONF_DIR "spark://$SPARK_MASTER_HOST:$SPARK_MASTER_PORT"
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/stop-slave.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/stop-slave.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/stop-slave.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/stop-slave.sh	2018-11-12 10:38:28.688466000 -0800
@@ -31,6 +31,23 @@ if [ -z "${SPARK_HOME}" ]; then
   export SPARK_HOME="$(cd "`dirname "$0"`"/..; pwd)"
 fi
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir="$1"
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR="$conf_dir"
+  fi
+  shift
+fi
+
 . "${SPARK_HOME}/sbin/spark-config.sh"
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
diff -pruN spark-2.4.0-bin-hadoop2.7-orig/sbin/stop-slaves.sh spark-2.4.0-bin-hadoop2.7-alternate/sbin/stop-slaves.sh
--- spark-2.4.0-bin-hadoop2.7-orig/sbin/stop-slaves.sh	2018-10-28 23:36:20.000000000 -0700
+++ spark-2.4.0-bin-hadoop2.7-alternate/sbin/stop-slaves.sh	2018-11-12 10:38:28.696440000 -0800
@@ -25,4 +25,4 @@ fi
 
 . "${SPARK_HOME}/bin/load-spark-env.sh"
 
-"${SPARK_HOME}/sbin/slaves.sh" cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin"/stop-slave.sh
+"${SPARK_HOME}/sbin/slaves.sh" --config $SPARK_CONF_DIR cd "${SPARK_HOME}" \; "${SPARK_HOME}/sbin"/stop-slave.sh --config $SPARK_CONF_DIR
