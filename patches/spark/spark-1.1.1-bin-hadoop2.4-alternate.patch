diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/slaves.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/slaves.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/slaves.sh	2016-08-16 15:05:27.030491000 -0700
@@ -25,6 +25,8 @@
 #     Default is ${SPARK_CONF_DIR}/slaves.
 #   SPARK_CONF_DIR  Alternate conf dir. Default is ${SPARK_HOME}/conf.
 #   SPARK_SLAVE_SLEEP Seconds to sleep between spawning remote commands.
+#   SPARK_SSH_CMD Specify an alternate remote shell command.
+#     Defaults to ssh if not specified.
 #   SPARK_SSH_OPTS Options passed to ssh when running remote commands.
 ##
 
@@ -73,13 +75,15 @@ if [ "$HOSTLIST" = "" ]; then
   fi
 fi
 
+RSH_CMD=${SPARK_SSH_CMD:-ssh}
+
 # By default disable strict host key checking
-if [ "$SPARK_SSH_OPTS" = "" ]; then
+if [ "$RSH_CMD" == "ssh" ] && [ "$SPARK_SSH_OPTS" = "" ]; then
   SPARK_SSH_OPTS="-o StrictHostKeyChecking=no"
 fi
 
 for slave in `cat "$HOSTLIST"|sed  "s/#.*$//;/^$/d"`; do
- ssh $SPARK_SSH_OPTS $slave $"${@// /\\ }" \
+ $RSH_CMD $SPARK_SSH_OPTS $slave $"${@// /\\ }" \
    2>&1 | sed "s/^/$slave: /" &
  if [ "$SPARK_SLAVE_SLEEP" != "" ]; then
    sleep $SPARK_SLAVE_SLEEP
diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/spark-daemon.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemon.sh	2016-08-16 15:05:27.032490000 -0700
@@ -137,7 +137,8 @@ case $startStop in
 
     if [ "$SPARK_MASTER" != "" ]; then
       echo rsync from $SPARK_MASTER
-      rsync -a -e ssh --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
+      RSH_CMD=${SPARK_SSH_CMD:-ssh}
+      rsync -a -e $RSH_CMD --delete --exclude=.svn --exclude='logs/*' --exclude='contrib/hod/logs/*' $SPARK_MASTER/ "$SPARK_HOME"
     fi
 
     spark_rotate_log "$log"
diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/spark-daemons.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/spark-daemons.sh	2016-08-16 15:05:27.035486000 -0700
@@ -30,6 +30,24 @@ fi
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
 . "$sbin/spark-config.sh"
 
-exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" "$@"
+exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/spark-daemon.sh" --config $SPARK_CONF_DIR "$@"
diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/start-slave.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/start-slave.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/start-slave.sh	2016-08-16 15:05:27.037486000 -0700
@@ -23,4 +23,27 @@
 sbin=`dirname "$0"`
 sbin=`cd "$sbin"; pwd`
 
-"$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+# Check if --config is passed as an argument. It is an optional parameter.
+# Exit if the argument is not a directory.
+
+if [ "$1" == "--config" ]
+then
+  shift
+  conf_dir=$1
+  if [ ! -d "$conf_dir" ]
+  then
+    echo "ERROR : $conf_dir is not a directory"
+    echo $usage
+    exit 1
+  else
+    export SPARK_CONF_DIR=$conf_dir
+  fi
+  shift
+fi
+
+if [ "${SPARK_CONF_DIR}X" != "X" ]
+then
+    "$sbin"/spark-daemon.sh --config $SPARK_CONF_DIR start org.apache.spark.deploy.worker.Worker "$@"
+else
+    "$sbin"/spark-daemon.sh start org.apache.spark.deploy.worker.Worker "$@"
+fi
diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/start-slaves.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/start-slaves.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/start-slaves.sh	2016-08-16 15:05:27.040480000 -0700
@@ -58,12 +58,12 @@ fi
 
 # Launch the slaves
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  exec "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" 1 spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
+  exec "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR 1 spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT
 else
   if [ "$SPARK_WORKER_WEBUI_PORT" = "" ]; then
     SPARK_WORKER_WEBUI_PORT=8081
   fi
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin/slaves.sh" cd "$SPARK_HOME" \; "$sbin/start-slave.sh" $(( $i + 1 ))  spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i ))
+    "$sbin/slaves.sh" --config $SPARK_CONF_DIR cd "$SPARK_HOME" \; "$sbin/start-slave.sh" --config $SPARK_CONF_DIR $(( $i + 1 ))  spark://$SPARK_MASTER_IP:$SPARK_MASTER_PORT --webui-port $(( $SPARK_WORKER_WEBUI_PORT + $i )) 
   done
 fi
diff -pruN spark-1.1.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh
--- spark-1.1.1-bin-hadoop2.4-orig/sbin/stop-slaves.sh	2014-11-19 13:08:09.000000000 -0800
+++ spark-1.1.1-bin-hadoop2.4-alternate-ssh/sbin/stop-slaves.sh	2016-08-16 15:05:27.042478000 -0700
@@ -30,9 +30,9 @@ if [ -e "$sbin"/../tachyon/bin/tachyon ]
 fi
 
 if [ "$SPARK_WORKER_INSTANCES" = "" ]; then
-  "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker 1
+  "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker 1
 else
   for ((i=0; i<$SPARK_WORKER_INSTANCES; i++)); do
-    "$sbin"/spark-daemons.sh stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
+    "$sbin"/spark-daemons.sh --config $SPARK_CONF_DIR stop org.apache.spark.deploy.worker.Worker $(( $i + 1 ))
   done
 fi
