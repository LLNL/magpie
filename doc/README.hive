Instructions For Running Hive
------------------------------

0) If necessary, download your favorite version of Hive from the
   Apache download site and install it into a location where it's
   accessible on all cluster nodes.  Usually this is on a NFS home
   directory.  You'll also need to download a binary version of
   PostgreSQL.

   Optionally, hive-testbench can be downoladed and used to run performance
   benchmarking tests that accompany the Hive software in Magpie (see below
   regarding patching hive-testbench prior to use).

   See 'Convenience Scripts' in README about
   misc/magpie-download-and-setup.sh, which may make the
   downloading and patching easier.

1) Select an appropriate submission script for running your job.  You
   can find them in the directory submission-scripts/, with Slurm
   Sbatch scripts using srun in script-sbatch-srun, Moab Msub+Slurm
   scripts using srun in script-msub-slurm-srun, Moab Msub+Torque
   scripts using pdsh in script-msub-torque-pdsh, and LSF scripts
   using mpirun in script-lsf-mpirun.

   You'll likely want to start with the base hive w/ hdfs script
   (e.g. magpie.sbatch-srun-hadoop-and-hive) for your
   scheduler/resource manager.  If you wish to configure more, you can
   choose to start with the base script (e.g. magpie.sbatch-srun)
   which contains all configuration options.

2) Setup your job essentials at the top of the submission script.  As
   an example, the following are the essentials for running with Moab.

   #MSUB -l nodes : Set how many nodes you want in your job

   #MSUB -l walltime : Set the time for this job to run

   #MSUB -l partition : Set the job partition

   #MSUB -q <my batch queue> : Set to batch queue

   MOAB_JOBNAME : Set your job name.

   MAGPIE_SCRIPTS_HOME : Set where your scripts are

   MAGPIE_LOCAL_DIR : For scratch space files

   MAGPIE_JOB_TYPE : This should be set to 'hive'

   JAVA_HOME : B/c you need to ...

3) Setup the essentials for Hive.

   HIVE_SETUP : Set to yes

   HIVE_VERSION : Set appropriately.

   HIVE_HOME : Where your Hive code is.  Typically in an NFS
   mount.

   HIVE_PORT : The port that HiveServer will listen for connections on

   HIVE_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

   HIVE_METASTORE_DIR : Path for Hive to use for the Hive Metastore

   HIVE_DB_NAME : Name of the database for this stack

   POSTGRES_HOME : Where PostgreSQL binary lives

   PGPORT : The port that PostgreSQL will listen for connections on. 11115 works well

   PGDATA : The path for PostgreSQL to use for its tables


4) Select how your job will run by setting HIVE_JOB.  The first time
   you'll probably want to run w/ 'testbench' mode just to try
   things out and make sure things look setup correctly.

   After this, you may want to run with 'interactive' mode to play
   around and figure things out.  In the job output you will see
   output similar to the following:

      ssh node70
      setenv JAVA_HOME "/usr/lib/jvm/jre-1.7.0-oracle.x86_64/"
      setenv HIVE_HOME "/home/username/apache-hive-2.3.0-bin"
      setenv HIVE_CONF_DIR "/tmp/username/hive/ajobtime/1174573/conf"

   These instructions will inform you how to login to the master node
   of your allocation and how to initialize your session.  Once in
   your session, you can do as you please.  For example, you can
   interact with the Hive database to start (bin/beeline).  There
   will also be instructions in your job output on how to tear the
   session down cleanly if you wish to end your job early.

   See "Exported Environment Variables" in README for information on
   common exported environment variables that may be useful in
   scripts.

   See below in "Hive Exported Environment Variables", for
   information on Hive specific exported environment variables that
   may be useful in scripts.

5) Hive requires HDFS, so ensure the Hadoop w/ HDFS is configured and
   also in your submission script.  MapReduce is not needed with Hive
   but can be setup along with it.  See README.hadoop for Hadoop setup
   instructions.

6) Hive requires Zookeeper, so setup the essentials for Zookeeper.
   See README.zookeeper for Zookeeper setup instructions.

7) Submit your job into the cluster by running "sbatch -k
   ./magpie.sbatchfile" for Slurm, "msub ./magpie.msubfile" for
   Moab, or "bsub < ./magpie.lsffile" for LSF.
   Add any other options you see fit.

8) Look at your job output file to see your output.  There will also
   be some notes/instructions/tips in the output file for viewing the
   status of your job in a web browser, environment variables you wish
   to set if interacting with it, etc.

   See "General Advanced Usage" in README for additional tips.

Hive Notes
-----------

Hive is configured to run on the master node, so increasing or decreasing
the number of nodes should not have an affect on Hive directly (it may
have an affect on performance, but Hive will always run on the master).

Hive Exported Environment Variables
------------------------------------

The following environment variables are exported when your job is run
and may be useful in scripts in your run or in pre/post run scripts.

HIVE_MASTER_NODE : The node where HiveSever and Hive Metastore are running

HIVE_HOME : Location to the Hive binaries

HIVE_PORT : The port that HiveServer is listening for connections on

HIVE_CONF_DIR : the directory that Hive configuration files local
                 to the node are stored.

POSTGRES_HOME : Where PostgreSQL binary lives

PGPORT : The port that PostgreSQL will listen for connections on. 11115 works well

PGDATA : The path for PostgreSQL to use for its tables


See "Hadoop Exported Environment Variables" in README.hadoop, for
Hadoop environment variables that may be useful.

See "Zookeeper Exported Environment Variables" in README.zookeeper,
for Zookeeper environment variables that may be useful.

Hive Patching
--------------

Typically there are no patches required for Hive to operate.  The only
contingency is that PostgreSQL is available on the system as well.

For additional functionality/testing purposes you may wish to download the
hive-testbench software from GitHub (https://github.com/hortonworks/hive-testbench.git)
and use git to patch the beeline versions of the scripts into the hive-testbench directory.
Prior to running Magpie in testbench mode, you must run the respective
setup.sh (either tpch-setup.sh or tpcds-setup.sh) scripts to build the
data generator for hive-testbench.  More information can be found in the
README in hive-testbench's Git repository.

There are some instances where the metastore and database paths
can become 'disjointed' Ex: the Metastore is configured for hdfs://node1/xxxx
however the paths within the database report hdfs://node2/xxxx.  Sourcing the
magpie-env.sh file and running the magpie-hive-update-table-paths.sh script should
resolve this situation.
