Instructions For Running Zookeeper
----------------------------------

0) If necessary, download your favorite version of Zookeeper off of
   Apache and install it into a location where it's accessible on all
   cluster nodes.  Usually this is on a NFS home directory.

   See 'Convenience Scripts' in README about
   misc/magpie-apache-download-and-setup.sh, which may make the
   downloading and patching easier.

1) Select an appropriate submission script for running your job.  You
   can find them in the directory submission-scripts/, with Slurm
   Sbatch scripts using srun in script-sbatch-srun, Moab Msub+Slurm
   scripts using srun in script-msub-slurm-srun, Moab Msub+Torque
   scripts using pdsh in script-msub-torque-pdsh, and LSF scripts
   using mpirun in script-lsf-mpirun.

   As Zookeeper is predominantly a node coordination service used by
   other services (e.g. Hbase, Storm), it's likely in the main
   submission scripts for those big data projects.  If the Zookeeper
   section isn't in it, just copy the Zookeeper section from base
   script (e.g. magpie.sbatch-srun) into it.

2) Setup your job essentials at the top of the submission script.  See
   other projects (e.g. Hbase in README.hbase, Storm in README.storm)
   for details on this setup.

   Be aware of how you many nodes you allocate for your job when
   running Zookeeper.  Zookeeper normally runs on cluster nodes
   separate from the rest (e.g. separate from nodes running HDFS or
   Hbase Regionservers).  So you may need to increase your node count.

   For example, if you desire 3 Zookeeper servers and 8 compute nodes,
   your total node count should be 12 (1 master, 8 compute, 3
   Zookeeper).

3) Setup the essentials for Zookeeper.

   ZOOKEEPER_SETUP : Set to yes

   ZOOKEEPER_VERSION : Set appropriately.

   ZOOKEEPER_HOME : Where your zookeeper code is.  Typically in an NFS
   mount.

   ZOOKEEPER_REPLICATION_COUNT : Number of nodes in your Zookeeper
   quorom.

   ZOOKEEPER_FILESYSTEM_MODE : most will likely want "networkfs" so
   data files can be stored to Lustre.  If you have local disks such
   as SSDs, you can use "local" instead, and set ZOOKEEPER_DATA_DIR to
   the local SSD path.

   ZOOKEEPER_DATA_DIR : The base path for where you will store
   Zookeeper data.  If a local SSD is available, it may be preferable
   to set this to a local drive and set ZOOKEEPER_FILESYSTEM_MODE
   above to "local".

   ZOOKEEPER_LOCAL_DIR : A small place for conf files and log files
   local to each node.  Typically /tmp directory.

4) Run your job as instructions dictate in other project sections
   (e.g. Hbase, Storm).

   See "Exported Environment Variables" in README for information on
   common exported environment variables that may be useful in
   scripts.

   See below in "Zookeeper Exported Environment Variables", for
   information on Zookeeper specific exported environment variables
   that may be useful in scripts.

   See "General Advanced Usage" in README for additional tips.

Zookeeper Exported Environment Variables
----------------------------------------

The following environment variables are exported when your job is run
and may be useful in scripts in your run or in pre/post run scripts.

ZOOKEEPER_NODES : comma separated list of Zookeeper nodes, may be
                  helpful as input to some tools / scripts

ZOOKEEPER_NODES_FIRST : first node in the ZOOKEEPER_NODES environment
                        variable, for convenience

ZOOKEEPER_CLIENT_PORT : port to communicate to Zookeeper

ZOOKEEPER_CONF_DIR : the directory that Zookeeper configuration files
          	     local to the node are stored.

ZOOKEEPER_LOG_DIR : the directory Zookeeper log files are stored
