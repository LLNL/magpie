Instructions For Using Tensorflow
---------------------------------

0) If necessary, download your favorite version of ray.  Make sure it
   is in the python path of the python you wish to use and that it is
   available on all nodes of your allocation (usually this means that
   it is installed in a NFS mounted path available on all nodes).

1) Select an appropriate submission script for running your job.  You
   can find them in the directory submission-scripts/, with Slurm
   Sbatch scripts using srun in script-sbatch-srun, Moab Msub+Slurm
   scripts using srun in script-msub-slurm-srun, Moab Msub+Torque
   scripts using pdsh in script-msub-torque-pdsh, LSF scripts using
   mpirun in script-lsf-mpirun, and Flux scripts in
   script-flux-batch-run.

   You'll likely want to start with the base ray script
   (e.g. magpie.sbatch-srun-ray) for your scheduler/resource manager.

2) Setup your job essentials at the top of the submission script.  As
   an example, the following are the essentials for running with Slurm.

   #SBATCH --nodes : Set how many nodes you want in your job

   #SBATCH --time : Set the time for this job to run

   #SBATCH --partition : Set the job partition, this is normally
                         pbatch or pdebug if you are debugging

   #SBATCH --job-name : Set your job name.

   MAGPIE_SCRIPTS_HOME : Set where your scripts are

   MAGPIE_LOCAL_DIR : For scratch space files

   MAGPIE_JOB_TYPE : This should be set to 'ray' initially

   MAGPIE_PYTHON : Set to the python you wish to run your tasks

4) Setup the essentials for Ray.

   RAY_SETUP : Set to yes

   RAY_PATH : Path to Ray executable.  Typically in an NFS mount.

   RAY_LOCAL_DIR : A small place for conf files and log files local
   to each node.  Typically /tmp directory.

3) Select how your job will run by setting MAGPIE_JOB_TYPE and/or
   RAY_JOB.  Initially, you'll likely want to set
   MAGPIE_JOB_TYPE to 'ray' and setting RAY_JOB to
   'rayips'.  This will allow you to run a pre-written job to make sure
   things are setup correctly.

   Once you have figured that out, you will likely want to set RAY_JOB
   to 'script' and set RAY_SCRIPT_PATH to a script you wish for Magpie
   to execute on your behalf via the python set by MAGPIE_PYTHON.

   See "Exported Environment Variables" in README for information on
   common exported environment variables that may be useful in
   scripts.

   See below in "Ray Exported Environment Variables", for
   information on Ray specific exported environment variables
   that are required for functionality.

6) Submit your job into the cluster by running "sbatch -k
   ./magpie.sbatchfile" for Slurm, "msub ./magpie.msubfile" for Moab,
   or "bsub < ./magpie.lsffile" for LSF.  Add any other options you
   see fit.

7) Look at your job output file to see your output.

Ray Exported Environment Variables
-----------------------------------------

The following environment variables are exported when your job is run
and will be required to be used by your ray job.

MAGPIE_RAY_ADDRESS

This environment variable holds the IP and port your script should
use to connect to the ray via ray.init().

The following environment variables are exported when your job is run
and may be useful.

MAGPIE_RAY_MASTER_NODE
MAGPIE_RAY_MASTER_NODE_IP_ADDRESS
MAGPIE_RAY_MASTER_PORT

These environment variables provide the master node hostname, ip
addresss, and port respectfully.

